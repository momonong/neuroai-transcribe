import os
import glob
import json
import shutil
from typing import List, Optional

# å¼•å…¥æª”æ¡ˆç®¡ç†å™¨å’Œå…¶ä»–æ¨¡çµ„
from .file_manager import file_manager
from .split import SmartAudioSplitter
from .pipeline import PipelinePhase2
from .stitch import run_stitching_logic
from .flag import run_anomaly_detector

def run_neuroai_pipeline(video_path: str, project_name: Optional[str] = None):
    """
    åŸ·è¡Œå®Œæ•´çš„ NeuroAI è½‰éŒ„æµç¨‹ï¼š
    1. Split (åˆ‡åˆ†)
    2. Process (Whisper + Pyannote + Alignment)
    3. Stitch (åˆä½µå¥å­)
    4. Flag (ç•°å¸¸æ¨™è¨˜)
    """
    # å»ºç«‹æˆ–å–å¾—å°ˆæ¡ˆ
    if project_name is None:
        project_name = file_manager.create_project(video_path)
    
    project_dir = file_manager.get_project_dir(project_name)
    chunks_dir = file_manager.get_temp_chunks_dir(project_name)
    
    print(f"ğŸš€ [AI Engine] å•Ÿå‹•æµç¨‹: {os.path.basename(video_path)}")
    print(f"ğŸ“‚ [AI Engine] å°ˆæ¡ˆ: {project_name}")
    print(f"ğŸ“ [AI Engine] å°ˆæ¡ˆè·¯å¾‘: {project_dir}")

    # ==========================================
    # Phase 1: åˆ‡åˆ†éŸ³è¨Š (Splitting)
    # ==========================================
    print("\nâœ‚ï¸ --- Phase 1: Audio Splitting ---")
    splitter = SmartAudioSplitter(output_dir=str(chunks_dir))
    # split_audio æœƒå›å‚³ metadata list
    chunk_metadata_list = splitter.split_audio(video_path, num_chunks=4)
    
    if not chunk_metadata_list:
        print("âŒ åˆ‡åˆ†å¤±æ•—ï¼Œæµç¨‹ä¸­æ­¢ã€‚")
        return

    # ==========================================
    # Phase 2: è¾¨è­˜èˆ‡å°é½Š (Processing)
    # ==========================================
    print("\nğŸ¤– --- Phase 2: Whisper & Diarization ---")
    
    # åˆå§‹åŒ–è™•ç†å™¨ (è¼‰å…¥æ¨¡å‹)
    processor = PipelinePhase2()
    
    all_aligned_segments = []

    # ä¾åºè™•ç†æ¯å€‹ chunk
    for chunk_meta in chunk_metadata_list:
        wav_path = chunk_meta['file_path']
        base_name = os.path.splitext(os.path.basename(wav_path))[0]
        
        # å®šç¾©ä¸­é–“ç”¢æª”å
        json_whisper = os.path.join(chunks_dir, f"{base_name}_whisper.json")
        json_diar = os.path.join(chunks_dir, f"{base_name}_diar.json")
        json_aligned = os.path.join(chunks_dir, f"{base_name}_aligned.json")
        
        # è¨ˆç®—åç§»é‡ (ç§’)
        offset_sec = chunk_meta['start_time_ms'] / 1000.0
        
        print(f"   Processing Chunk: {base_name} (Offset: {offset_sec}s)")

        # 1. è·‘ Whisper
        processor.run_whisper(wav_path, json_whisper)
        
        # 2. è·‘ Pyannote
        processor.run_diarization(wav_path, json_diar)
        
        # 3. è·‘å°é½Š (Alignment)
        processor.run_alignment(json_whisper, json_diar, json_aligned, chunk_offset_sec=offset_sec)
        
        # 4. è®€å–å°é½ŠçµæœåŠ å…¥ç¸½è¡¨
        if os.path.exists(json_aligned):
            with open(json_aligned, 'r', encoding='utf-8') as f:
                segments = json.load(f)
                all_aligned_segments.extend(segments)

    # é‡‹æ”¾ GPU è¨˜æ†¶é«” (é‡è¦ï¼)
    del processor
    import torch
    import gc
    gc.collect()
    torch.cuda.empty_cache()

    # å„²å­˜æœªä¿®é£¾çš„åŸå§‹è½‰éŒ„æª” (å‚™ä»½ç”¨)
    raw_path = file_manager.get_output_file_path(project_name, "raw_aligned_transcript.json")
    # ä¾æ™‚é–“æ’åº
    all_aligned_segments.sort(key=lambda x: x['start'])
    file_manager.save_json(all_aligned_segments, raw_path, backup=False)

    # ==========================================
    # Phase 3: å¥å­ä¿®å¾© (Stitching)
    # ==========================================
    print("\nğŸ”— --- Phase 3: Stitching & Correction ---")
    # å‘¼å« stitch.py çš„é‚è¼¯
    stitched_data = run_stitching_logic(all_aligned_segments)

    # ==========================================
    # Phase 4: ç•°å¸¸æ¨™è¨˜ (Flagging)
    # ==========================================
    print("\nğŸš© --- Phase 4: Anomaly Detection ---")
    # å‘¼å« flag.py çš„é‚è¼¯
    final_data = run_anomaly_detector(stitched_data)

    # ==========================================
    # Final: è¼¸å‡ºæœ€çµ‚çµæœ
    # ==========================================
    final_output_path = file_manager.get_output_file_path(project_name, "transcript.json")
    file_manager.save_json(final_data, final_output_path, backup=True)

    print(f"\nâœ…âœ…âœ… Pipeline Complete! Result saved to: {final_output_path}")
    
    # æ¸…ç†æš«å­˜æª” (å¯é¸)
    # shutil.rmtree(chunks_dir) 
    
    return str(final_output_path)