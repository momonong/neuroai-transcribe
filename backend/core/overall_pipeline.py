#!/usr/bin/env python3
"""
å®Œæ•´çš„ NeuroAI è½‰éŒ„æµç¨‹ (å„ªåŒ–ç‰ˆ)
åŒ…å«è‡ªå‹•è¨˜æ†¶é«”ç®¡ç†èˆ‡è­¦å‘Šéæ¿¾ï¼Œè§£æ±º GPU å¡æ­»å•é¡Œ
"""
import os
import sys
import json
import glob
import gc
import torch
import warnings
from pathlib import Path
from typing import Optional, List, Dict, Any
from datetime import datetime

# ==========================================
# 1. éæ¿¾æƒ±äººçš„è­¦å‘Š (Clean Logs)
# ==========================================
# å¿½ç•¥ Pyannote/Torchaudio çš„ç‰ˆæœ¬æ£„ç”¨è­¦å‘Š
warnings.filterwarnings("ignore", category=UserWarning)
warnings.filterwarnings("ignore", category=FutureWarning)
# ç‰¹åˆ¥é‡å° TF32 å’Œ torchaudio backend çš„è­¦å‘Š
warnings.filterwarnings("ignore", message=".*torchaudio._backend.*")
warnings.filterwarnings("ignore", message=".*TensorFloat-32.*")
warnings.filterwarnings("ignore", message=".*degrees of freedom.*")

# ç¢ºä¿èƒ½æ‰¾åˆ° core æ¨¡çµ„
current_dir = Path(__file__).parent
backend_dir = current_dir.parent
sys.path.insert(0, str(backend_dir))

from core.config import config
from core.file_manager import file_manager
from core.split import SmartAudioSplitter
from core.pipeline import PipelinePhase2
from core.stitch import run_stitching_logic
from core.flag import run_anomaly_detector

class OverallPipeline:
    """å®Œæ•´çš„è½‰éŒ„æµç¨‹ç®¡ç†å™¨"""
    
    def __init__(self, video_path: str, case_name: Optional[str] = None, force_reprocess: bool = False):
        """
        åˆå§‹åŒ–å®Œæ•´æµç¨‹
        """
        self.video_path = Path(video_path)
        self.force_reprocess = force_reprocess
        
        if not self.video_path.exists():
            raise FileNotFoundError(f"æ‰¾ä¸åˆ°å½±ç‰‡æª”æ¡ˆ: {video_path}")
        
        # æ±ºå®šæ¡ˆä¾‹åç¨±
        if case_name is None:
            # å¾æª”åè‡ªå‹•ç”Ÿæˆæ¡ˆä¾‹åç¨±
            timestamp = datetime.now().strftime("%Y%m%d-%H%M")
            video_name = self.video_path.stem
            self.case_name = f"{timestamp}-{video_name}"
        else:
            self.case_name = case_name
        
        # è¨­å®šæ¡ˆä¾‹ç›®éŒ„
        self.case_dir = file_manager.get_case_dir(self.case_name)
        
        # åˆå§‹åŒ–å„å€‹è™•ç†å™¨
        self.splitter = SmartAudioSplitter(case_name=self.case_name)
        self.ai_processor = PipelinePhase2()
        
        print(f"ğŸ¬ åˆå§‹åŒ–å®Œæ•´æµç¨‹")
        print(f"   ğŸ“„ å½±ç‰‡æª”æ¡ˆ:  {self.video_path.name}")
        print(f"   ğŸ“ æ¡ˆä¾‹åç¨±: {self.case_name}")
        print(f"   ğŸ“‚ è¼¸å‡ºç›®éŒ„: {self.case_dir}")
        print(f"   ğŸ”„ å¼·åˆ¶é‡æ–°è™•ç†: {'æ˜¯' if force_reprocess else 'å¦'}")

    def _clean_gpu(self):
        """
        å¼·åˆ¶æ¸…ç† GPU è¨˜æ†¶é«”èˆ‡åŒæ­¥
        é€™æ˜¯è§£æ±º Pipeline å¡æ­»çš„é—œéµå‡½æ•¸
        """
        gc.collect() # æ¸…é™¤ Python ç„¡ç”¨è®Šæ•¸
        if torch.cuda.is_available():
            torch.cuda.synchronize() # ç­‰å¾…æ‰€æœ‰ GPU ä»»å‹™å®Œæˆ
            torch.cuda.empty_cache() # é‡‹æ”¾é¡¯å­˜
            torch.cuda.synchronize() # å†æ¬¡ç¢ºèª
        # print("   ğŸ§¹ VRAM Cleaned & Synced.") 

    def step1_split_audio(self, num_chunks: int = 4) -> List[Dict[str, Any]]:
        """æ­¥é©Ÿ 1: éŸ³è¨Šåˆ‡åˆ†"""
        print(f"\nğŸ”ª [æ­¥é©Ÿ 1/6] éŸ³è¨Šåˆ‡åˆ†...")
        print("=" * 50)
        
        # æª¢æŸ¥æ˜¯å¦å·²æœ‰ chunk æª”æ¡ˆ
        existing_chunks = list(self.case_dir.glob("chunk_*.wav"))
        
        if existing_chunks and not self.force_reprocess:
            print(f"â© ç™¼ç¾ {len(existing_chunks)} å€‹ç¾æœ‰ chunk æª”æ¡ˆï¼Œè·³éåˆ‡åˆ†æ­¥é©Ÿ")
            
            # å¾ç¾æœ‰æª”æ¡ˆå»ºç«‹ metadata
            chunk_metadata = []
            for chunk_file in sorted(existing_chunks):
                try:
                    # è§£ææª”åå–å¾—æ™‚é–“è³‡è¨Š
                    parts = chunk_file.stem.split('_')
                    if len(parts) >= 4:
                        start_ms = int(parts[-2])
                        end_ms = int(parts[-1])
                    else:
                        start_ms = 0
                        end_ms = 60000  # é è¨­å€¼
                    
                    chunk_metadata.append({
                        'file_path': str(chunk_file),
                        'start_time_ms': start_ms,
                        'end_time_ms': end_ms,
                        'duration_ms': end_ms - start_ms,
                        'chunk_id': len(chunk_metadata) + 1
                    })
                except Exception as e:
                    print(f"   âš ï¸ è§£ææª”åå¤±æ•— {chunk_file.name}: {e}")
                    continue
            
            for chunk in chunk_metadata:
                duration_sec = chunk['duration_ms'] / 1000
                print(f"   - {Path(chunk['file_path']).name}: {duration_sec:.1f}s")
            
            return chunk_metadata
        
        try:
            # å¦‚æœå¼·åˆ¶é‡æ–°è™•ç†ï¼Œå…ˆæ¸…ç†èˆŠæª”æ¡ˆ
            if self.force_reprocess and existing_chunks:
                print(f"ğŸ—‘ï¸ æ¸…ç† {len(existing_chunks)} å€‹èˆŠ chunk æª”æ¡ˆ...")
                for chunk_file in existing_chunks:
                    chunk_file.unlink()
                    # åŒæ™‚æ¸…ç†ç›¸é—œçš„ JSON æª”æ¡ˆ
                    base_name = chunk_file.stem
                    for suffix in ['_whisper.json', '_diar.json', '_aligned.json']:
                        json_file = chunk_file.parent / f"{base_name}{suffix}"
                        if json_file.exists():
                            json_file.unlink()
            
            # åŸ·è¡ŒéŸ³è¨Šåˆ‡åˆ†
            chunk_metadata = self.splitter.split_audio(
                str(self.video_path), 
                num_chunks=num_chunks
            )
            
            print(f"âœ… éŸ³è¨Šåˆ‡åˆ†å®Œæˆï¼Œç”¢ç”Ÿ {len(chunk_metadata)} å€‹ç‰‡æ®µ")
            for chunk in chunk_metadata:
                duration_sec = chunk['duration_ms'] / 1000
                print(f"   - {Path(chunk['file_path']).name}: {duration_sec:.1f}s")
            
            return chunk_metadata
            
        except Exception as e:
            print(f"âŒ éŸ³è¨Šåˆ‡åˆ†å¤±æ•—: {e}")
            raise
    
    def step2_ai_processing(self, chunk_metadata: List[Dict[str, Any]]) -> List[str]:
        """æ­¥é©Ÿ 2-4: AI è™•ç† (Whisper + Diarization + Alignment)"""
        print(f"\nğŸ¤– [æ­¥é©Ÿ 2-4/6] AI è™•ç† (Whisper + Diarization + Alignment)...")
        print("=" * 50)
        
        aligned_files = []
        success_count = 0
        
        for i, chunk_info in enumerate(chunk_metadata):
            chunk_path = chunk_info['file_path']
            chunk_name = Path(chunk_path).name
            
            print(f"\nğŸ”„ [{i+1}/{len(chunk_metadata)}] è™•ç†: {chunk_name}")
            
            try:
                # æº–å‚™æª”æ¡ˆè·¯å¾‘
                base_path = Path(chunk_path).with_suffix('')
                whisper_json = f"{base_path}_whisper.json"
                diar_json = f"{base_path}_diar.json"
                aligned_json = f"{base_path}_aligned.json"
                
                # æª¢æŸ¥æ˜¯å¦å·²å®Œæˆè™•ç†
                if os.path.exists(aligned_json) and not self.force_reprocess:
                    print(f"   â© å·²è™•ç†å®Œæˆï¼Œè·³é: {chunk_name}")
                    aligned_files.append(aligned_json)
                    success_count += 1
                    continue
                
                # å¦‚æœå¼·åˆ¶é‡æ–°è™•ç†ï¼Œæ¸…ç†èˆŠæª”æ¡ˆ
                if self.force_reprocess:
                    for json_file in [whisper_json, diar_json, aligned_json]:
                        if os.path.exists(json_file):
                            os.remove(json_file)
                            print(f"   ğŸ—‘ï¸ æ¸…ç†èˆŠæª”æ¡ˆ: {Path(json_file).name}")
                
                # è¨ˆç®—æ™‚é–“åç§»
                start_ms = chunk_info['start_time_ms']
                offset_sec = start_ms / 1000.0
                
                # ==========================================
                # Phase 1: Whisper
                # ==========================================
                if not os.path.exists(whisper_json):
                    print(f"   ğŸ§ åŸ·è¡Œ Whisper...")
                    self.ai_processor.run_whisper(chunk_path, whisper_json)
                    # ğŸ”¥ é—œéµï¼šè·‘å®Œä¸€å€‹æ¨¡å‹é¦¬ä¸Šæ¸…è¨˜æ†¶é«”ï¼Œé¿å…è·Ÿä¸‹ä¸€å€‹æ¨¡å‹æ‰“æ¶
                    self._clean_gpu()
                else:
                    print(f"   â­ï¸ Whisper å·²å­˜åœ¨ï¼Œè·³éã€‚")
                
                # ==========================================
                # Phase 2: Diarization
                # ==========================================
                if not os.path.exists(diar_json):
                    print(f"   ğŸ—£ï¸ åŸ·è¡Œ Diarization...")
                    self.ai_processor.run_diarization(chunk_path, diar_json)
                    # ğŸ”¥ é—œéµï¼šå†æ¸…ä¸€æ¬¡
                    self._clean_gpu()
                else:
                    print(f"   â­ï¸ Diarization å·²å­˜åœ¨ï¼Œè·³éã€‚")
                
                # ==========================================
                # Phase 3: Alignment
                # ==========================================
                print(f"   ğŸ”— åŸ·è¡Œ Alignment...")
                self.ai_processor.run_alignment(whisper_json, diar_json, aligned_json, offset_sec)
                
                aligned_files.append(aligned_json)
                success_count += 1
                print(f"   âœ… å®Œæˆ: {chunk_name}")
                
            except KeyboardInterrupt:
                print(f"\nâš ï¸ ä½¿ç”¨è€…ä¸­æ–·è™•ç†")
                print(f"ğŸ’¡ å·²è™•ç†çš„æª”æ¡ˆæœƒä¿ç•™ï¼Œä¸‹æ¬¡åŸ·è¡Œæ™‚æœƒè‡ªå‹•è·³é")
                break
            except Exception as e:
                print(f"   âŒ è™•ç†å¤±æ•— {chunk_name}: {e}")
                import traceback
                traceback.print_exc()
                continue
            finally:
                # ç¢ºä¿æ¯å€‹è¿´åœˆçµæŸéƒ½åšä¸€æ¬¡å¾¹åº•æ¸…ç†
                self._clean_gpu()
        
        print(f"\nâœ… AI è™•ç†å®Œæˆ: {success_count}/{len(chunk_metadata)} å€‹ç‰‡æ®µæˆåŠŸ")
        return aligned_files
    
    def step3_merge_chunks(self, aligned_files: List[str]) -> List[Dict[str, Any]]:
        """æ­¥é©Ÿ 5: åˆä½µæ‰€æœ‰ chunk çš„çµæœ"""
        print(f"\nğŸ”— [æ­¥é©Ÿ 5/6] åˆä½µç‰‡æ®µ...")
        print("=" * 50)
        
        all_segments = []
        
        for aligned_file in aligned_files:
            if not os.path.exists(aligned_file):
                print(f"âš ï¸ è·³éä¸å­˜åœ¨çš„æª”æ¡ˆ: {aligned_file}")
                continue
            
            try:
                with open(aligned_file, 'r', encoding='utf-8') as f:
                    segments = json.load(f)
                
                # ç¢ºä¿æ¯å€‹ segment éƒ½æœ‰å¿…è¦çš„æ¬„ä½
                for segment in segments:
                    if 'sentence_id' not in segment:
                        segment['sentence_id'] = len(all_segments)
                    if 'verification_score' not in segment:
                        segment['verification_score'] = 1.0
                    if 'status' not in segment:
                        segment['status'] = 'auto'
                    if 'needs_review' not in segment:
                        segment['needs_review'] = False
                    if 'review_reason' not in segment:
                        segment['review_reason'] = None
                
                all_segments.extend(segments)
                print(f"   ğŸ“„ è¼‰å…¥ {len(segments)} å€‹ç‰‡æ®µå¾ {Path(aligned_file).name}")
                
            except Exception as e:
                print(f"   âŒ è¼‰å…¥å¤±æ•— {aligned_file}: {e}")
                continue
        
        # é‡æ–°ç·¨è™Ÿ sentence_id
        for i, segment in enumerate(all_segments):
            segment['sentence_id'] = i
        
        print(f"âœ… åˆä½µå®Œæˆï¼Œç¸½å…± {len(all_segments)} å€‹ç‰‡æ®µ")
        return all_segments
    
    def step4_stitch_and_flag(self, segments: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """æ­¥é©Ÿ 6: æ–‡å­—æ•´ç†å’Œç•°å¸¸æ¨™è¨˜"""
        print(f"\nğŸ§µ [æ­¥é©Ÿ 6/6] æ–‡å­—æ•´ç†å’Œç•°å¸¸æ¨™è¨˜...")
        print("=" * 50)
        
        try:
            # åŸ·è¡Œæ–‡å­—æ•´ç† (Stitch)
            print("ğŸ§µ åŸ·è¡Œæ–‡å­—æ•´ç†...")
            stitched_segments = run_stitching_logic(segments)
            print(f"   âœ… æ–‡å­—æ•´ç†å®Œæˆ: {len(stitched_segments)} å€‹å¥å­")
            
            # åŸ·è¡Œç•°å¸¸æ¨™è¨˜ (Flag)
            print("ğŸš© åŸ·è¡Œç•°å¸¸æ¨™è¨˜...")
            flagged_segments = run_anomaly_detector(stitched_segments)
            
            # çµ±è¨ˆæ¨™è¨˜çµæœ
            flagged_count = sum(1 for seg in flagged_segments if seg.get('needs_review', False))
            print(f"   âœ… ç•°å¸¸æ¨™è¨˜å®Œæˆ: {flagged_count} å€‹ç‰‡æ®µéœ€è¦äººå·¥æª¢æŸ¥")
            
            return flagged_segments
            
        except Exception as e:
            print(f"âŒ æ–‡å­—æ•´ç†å’Œæ¨™è¨˜å¤±æ•—: {e}")
            # å¦‚æœå¤±æ•—ï¼Œè¿”å›åŸå§‹ç‰‡æ®µ
            return segments
    
    def save_results(self, final_segments: List[Dict[str, Any]]) -> str:
        """å„²å­˜æœ€çµ‚çµæœ"""
        print(f"\nğŸ’¾ å„²å­˜æœ€çµ‚çµæœ...")
        print("=" * 30)
        
        # æº–å‚™æœ€çµ‚è³‡æ–™çµæ§‹
        final_data = {
            "case_name": self.case_name,
            "video_file": self.video_path.name,
            "processed_at": datetime.now().isoformat(),
            "total_segments": len(final_segments),
            "flagged_segments": sum(1 for seg in final_segments if seg.get('needs_review', False)),
            "speaker_mapping": {},  # å¯ä»¥å¾ŒçºŒæ‰‹å‹•ç·¨è¼¯
            "segments": final_segments
        }
        
        # å„²å­˜åˆ°æ¡ˆä¾‹ç›®éŒ„
        output_file = self.case_dir / "final_transcript.json"
        
        try:
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(final_data, f, ensure_ascii=False, indent=2)
            
            print(f"âœ… çµæœå·²å„²å­˜: {output_file}")
            
            # é¡¯ç¤ºçµ±è¨ˆè³‡è¨Š
            total_duration = max(seg['end'] for seg in final_segments) if final_segments else 0
            flagged_count = final_data['flagged_segments']
            
            print(f"ğŸ“Š è™•ç†çµ±è¨ˆ:")
            print(f"   ğŸ¬ å½±ç‰‡é•·åº¦: {total_duration:.1f} ç§’")
            print(f"   ğŸ“ ç¸½ç‰‡æ®µæ•¸: {len(final_segments)}")
            print(f"   ğŸš© éœ€æª¢æŸ¥ç‰‡æ®µ: {flagged_count}")
            print(f"   âœ… è‡ªå‹•é€šé: {len(final_segments) - flagged_count}")
            
            return str(output_file)
            
        except Exception as e:
            print(f"âŒ å„²å­˜å¤±æ•—: {e}")
            raise
    
    def run_complete_pipeline(self, num_chunks: int = 4) -> str:
        """åŸ·è¡Œå®Œæ•´æµç¨‹"""
        print(f"ğŸš€ é–‹å§‹å®Œæ•´è½‰éŒ„æµç¨‹")
        print(f"ğŸ“¹ å½±ç‰‡: {self.video_path}")
        print(f"ğŸ“ æ¡ˆä¾‹: {self.case_name}")
        print(f"ğŸ”„ æ¨¡å¼: {'å¼·åˆ¶é‡æ–°è™•ç†' if self.force_reprocess else 'æ–·é»çºŒå‚³'}")
        print("=" * 60)
        
        start_time = datetime.now()
        
        try:
            # æ­¥é©Ÿ 1: éŸ³è¨Šåˆ‡åˆ†
            chunk_metadata = self.step1_split_audio(num_chunks)
            
            # æ­¥é©Ÿ 2-4: AI è™•ç†
            aligned_files = self.step2_ai_processing(chunk_metadata)
            
            if not aligned_files:
                raise Exception("æ²’æœ‰æˆåŠŸè™•ç†çš„éŸ³è¨Šç‰‡æ®µ")
            
            # æ­¥é©Ÿ 5: åˆä½µç‰‡æ®µ
            all_segments = self.step3_merge_chunks(aligned_files)
            
            if not all_segments:
                raise Exception("æ²’æœ‰å¯ç”¨çš„è½‰éŒ„ç‰‡æ®µ")
            
            # æ­¥é©Ÿ 6: æ–‡å­—æ•´ç†å’Œæ¨™è¨˜
            final_segments = self.step4_stitch_and_flag(all_segments)
            
            # å„²å­˜çµæœ
            output_file = self.save_results(final_segments)
            
            # è¨ˆç®—ç¸½è€—æ™‚
            end_time = datetime.now()
            duration = end_time - start_time
            
            print("\n" + "=" * 60)
            print("ğŸ‰ å®Œæ•´æµç¨‹åŸ·è¡ŒæˆåŠŸï¼")
            print(f"â±ï¸  ç¸½è€—æ™‚: {duration}")
            print(f"ğŸ“„ çµæœæª”æ¡ˆ: {output_file}")
            print("=" * 60)
            
            return output_file
            
        except KeyboardInterrupt:
            print(f"\nâš ï¸ æµç¨‹è¢«ä½¿ç”¨è€…ä¸­æ–·")
            print(f"ğŸ’¡ å·²è™•ç†çš„æª”æ¡ˆæœƒä¿ç•™ï¼Œå¯ä»¥ä½¿ç”¨ç›¸åŒå‘½ä»¤ç¹¼çºŒåŸ·è¡Œ")
            raise
        except Exception as e:
            print(f"\nâŒ æµç¨‹åŸ·è¡Œå¤±æ•—: {e}")
            import traceback
            traceback.print_exc()
            raise


def main():
    """ä¸»ç¨‹å¼å…¥å£"""
    import argparse
    
    parser = argparse.ArgumentParser(description="NeuroAI å®Œæ•´è½‰éŒ„æµç¨‹")
    parser.add_argument("video_path", help="MP4 å½±ç‰‡æª”æ¡ˆè·¯å¾‘")
    parser.add_argument("--case-name", help="æ¡ˆä¾‹åç¨± (å¯é¸ï¼Œé è¨­è‡ªå‹•ç”Ÿæˆ)")
    parser.add_argument("--chunks", type=int, default=4, help="éŸ³è¨Šåˆ‡åˆ†ç‰‡æ®µæ•¸ (é è¨­: 4)")
    parser.add_argument("--force", action="store_true", help="å¼·åˆ¶é‡æ–°è™•ç†æ‰€æœ‰æª”æ¡ˆ")
    
    args = parser.parse_args()
    
    try:
        # å»ºç«‹ä¸¦åŸ·è¡Œæµç¨‹
        pipeline = OverallPipeline(args.video_path, args.case_name, force_reprocess=args.force)
        result_file = pipeline.run_complete_pipeline(args.chunks)
        
        print(f"\nâœ… è½‰éŒ„å®Œæˆï¼çµæœæª”æ¡ˆ: {result_file}")
        
    except KeyboardInterrupt:
        print(f"\nâš ï¸ ç¨‹å¼è¢«ä¸­æ–·")
        print(f"ğŸ’¡ æç¤ºï¼šä¸‹æ¬¡åŸ·è¡Œç›¸åŒå‘½ä»¤æœƒè‡ªå‹•å¾ä¸­æ–·è™•ç¹¼çºŒ")
        print(f"ğŸ’¡ å¦‚è¦é‡æ–°é–‹å§‹ï¼Œè«‹åŠ ä¸Š --force åƒæ•¸")
        sys.exit(1)
    except Exception as e:
        print(f"\nâŒ åŸ·è¡Œå¤±æ•—: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()