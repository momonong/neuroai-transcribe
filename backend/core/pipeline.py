import os
import glob
import json
import torch
import pathlib
import warnings
import gc
from dotenv import load_dotenv

# å¼•å…¥é…ç½®
from core.config import config

# --- 1. è¨­å®šç’°å¢ƒ ---
load_dotenv()
MODEL_ROOT = config.model_cache_dir
os.environ["HF_HOME"] = MODEL_ROOT
os.environ["HF_HUB_DISABLE_SYMLINKS_WARNING"] = "1"

# --- 2. ğŸ›¡ï¸ RTX 5090 / PyTorch 2.8+ çµ‚æ¥µç™½åå–®è£œä¸ ---

print(f"ğŸ”§ Applying PyTorch {torch.__version__} security patches...")

try:
    # åŒ¯å…¥ Pyannote ä»»å‹™å®šç¾©æ¨¡çµ„
    import pyannote.audio.core.task
    from torch.torch_version import TorchVersion
    
    # å®šç¾©æˆ‘å€‘éœ€è¦è§£é–çš„é¡åˆ¥åç¨±æ¸…å–® (é€™æ˜¯ Pyannote æ¨¡å‹çš„æ ¸å¿ƒä¸‰å·¨é ­)
    target_classes = ["Specifications", "Problem", "Resolution"]
    
    safe_list = [TorchVersion, pathlib.PosixPath, pathlib.WindowsPath]
    
    # å‹•æ…‹æŠ“å– Pyannote çš„é¡åˆ¥
    for name in target_classes:
        if hasattr(pyannote.audio.core.task, name):
            cls = getattr(pyannote.audio.core.task, name)
            safe_list.append(cls)
            print(f"   -> Found and added to safelist: {name}")
    
    # è¨»å†Šç™½åå–®
    if hasattr(torch.serialization, "add_safe_globals"):
        torch.serialization.add_safe_globals(safe_list)
        print("âœ… Safe globals registered successfully.")
        
except ImportError as e:
    print(f"âš ï¸ Patch warning: Could not import pyannote modules ({e})")
except Exception as e:
    print(f"âš ï¸ Patch warning: {e}")

# å†æ¬¡å¼·åˆ¶ Patch torch.load (é›™é‡ä¿éšª)
original_load = torch.load
def permissive_load(*args, **kwargs):
    if 'weights_only' not in kwargs:
        kwargs['weights_only'] = False
    return original_load(*args, **kwargs)
torch.load = permissive_load

from typing import Optional
# --- 3. åŒ¯å…¥é‡å‹å¥—ä»¶ ---
from faster_whisper import WhisperModel
from pyannote.audio import Pipeline

class PipelinePhase2:
    def __init__(self, device: Optional[str] = None):
        self.device = device or config.device
        self.compute_type = config.compute_type if torch.cuda.is_available() else "int8"
        
        if not config.hf_token:
            print("âš ï¸ è­¦å‘Š: æœªåµæ¸¬åˆ° HF_TOKENï¼ŒPyannote å¯èƒ½æœƒå ±éŒ¯ã€‚")

    # --- Step 1: Whisper ---
    def run_whisper(self, audio_path, output_json_path):
        if os.path.exists(output_json_path):
            print(f"â© Whisper output exists, skipping.")
            return

        print(f"ğŸ§ [Step 1] Running Whisper on {os.path.basename(audio_path)}...")
        
        # æª¢æŸ¥éŸ³è¨Šæª”æ¡ˆå¤§å°ï¼Œçµ¦å‡ºé ä¼°æ™‚é–“
        try:
            file_size_mb = os.path.getsize(audio_path) / (1024 * 1024)
            estimated_minutes = file_size_mb / 10  # ç²—ç•¥ä¼°è¨ˆï¼šæ¯ 10MB ç´„éœ€ 1 åˆ†é˜
            print(f"   ğŸ“Š æª”æ¡ˆå¤§å°: {file_size_mb:.1f}MB, é ä¼°è™•ç†æ™‚é–“: {estimated_minutes:.1f} åˆ†é˜")
        except:
            pass
        
        try:
            model = WhisperModel(
                config.whisper_model, 
                device=self.device, 
                compute_type=self.compute_type,
                download_root=MODEL_ROOT 
            )

            print(f"   ğŸ”„ é–‹å§‹è½‰éŒ„... (é€™å¯èƒ½éœ€è¦å¹¾åˆ†é˜)")
            segments, info = model.transcribe(
                audio_path,
                beam_size=config.whisper_beam_size,
                word_timestamps=True,
                vad_filter=True,
                language=config.whisper_language
            )
            
            print(f"   ğŸ“ è™•ç†è½‰éŒ„çµæœ...")
            results = []
            segment_count = 0
            
            for seg in segments:
                results.append({
                    "start": seg.start,
                    "end": seg.end,
                    "text": seg.text,
                    "words": [{"start": w.start, "end": w.end, "word": w.word} for w in seg.words] if seg.words else []
                })
                segment_count += 1
                
                # æ¯è™•ç† 50 å€‹ç‰‡æ®µé¡¯ç¤ºä¸€æ¬¡é€²åº¦
                if segment_count % 50 == 0:
                    print(f"   ğŸ“Š å·²è™•ç† {segment_count} å€‹ç‰‡æ®µ...")
            
            print(f"   ğŸ’¾ å„²å­˜çµæœ... (å…± {len(results)} å€‹ç‰‡æ®µ)")
            with open(output_json_path, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            
            print(f"âœ… Whisper done. Saved to {output_json_path}")

        except Exception as e:
            print(f"âŒ Whisper è™•ç†å¤±æ•—: {e}")
            # å¦‚æœæœ‰éƒ¨åˆ†çµæœï¼Œå˜—è©¦å„²å­˜
            if 'results' in locals() and results:
                print(f"   ğŸ”„ å˜—è©¦å„²å­˜éƒ¨åˆ†çµæœ...")
                try:
                    with open(output_json_path, 'w', encoding='utf-8') as f:
                        json.dump(results, f, ensure_ascii=False, indent=2)
                    print(f"   âœ… éƒ¨åˆ†çµæœå·²å„²å­˜")
                except:
                    pass
            raise
        finally:
            # å¾¹åº•é‡‹æ”¾è¨˜æ†¶é«”çš„ä¸‰é€£æ“Š
            if 'model' in locals():
                del model
            gc.collect() # å¼·åˆ¶ Python å›æ”¶è¨˜æ†¶é«”ç‰©ä»¶
            if torch.cuda.is_available():
                torch.cuda.empty_cache() # å¼·åˆ¶ PyTorch é‡‹æ”¾ VRAM
            print("ğŸ§¹ VRAM cleaned.")

    # --- Step 2: Pyannote ---
    def run_diarization(self, audio_path, output_json_path):
        if os.path.exists(output_json_path):
            print(f"â© Diarization output exists, skipping.")
            return

        print(f"ğŸ—£ï¸ [Step 2] Running Pyannote on {os.path.basename(audio_path)}...")
        
        hf_token = config.hf_token
        try:
            print(f"   ğŸ”„ è¼‰å…¥ Diarization æ¨¡å‹...")
            pipeline = Pipeline.from_pretrained(
                "pyannote/speaker-diarization-3.1",
                use_auth_token=hf_token,
                cache_dir=MODEL_ROOT
            ).to(torch.device(self.device))
            
            print(f"   ğŸ”„ åŸ·è¡Œèªªè©±è€…åˆ†é›¢... (é€™å¯èƒ½éœ€è¦å¹¾åˆ†é˜)")
            diarization = pipeline(audio_path)

        except Exception as e:
            print(f"âŒ Pyannote loading failed: {e}")
            # å¦‚æœé‚„ç¼ºä»€éº¼ï¼Œé¡¯ç¤ºå‡ºä¾†æ–¹ä¾¿é™¤éŒ¯
            import traceback
            traceback.print_exc()
            return

        try:
            print(f"   ğŸ“ è™•ç†åˆ†é›¢çµæœ...")
            diar_segments = []
            for turn, _, speaker in diarization.itertracks(yield_label=True):
                diar_segments.append({
                    "start": turn.start,
                    "end": turn.end,
                    "speaker": speaker
                })
            
            print(f"   ğŸ’¾ å„²å­˜çµæœ... (å…± {len(diar_segments)} å€‹èªªè©±ç‰‡æ®µ)")
            with open(output_json_path, 'w', encoding='utf-8') as f:
                json.dump(diar_segments, f, ensure_ascii=False, indent=2)

            print(f"âœ… Diarization done. Saved to {output_json_path}")

        except Exception as e:
            print(f"âŒ Diarization çµæœè™•ç†å¤±æ•—: {e}")
            raise
        finally:
            # å¾¹åº•é‡‹æ”¾è¨˜æ†¶é«”çš„ä¸‰é€£æ“Š
            if 'pipeline' in locals():
                del pipeline
            gc.collect() # å¼·åˆ¶ Python å›æ”¶è¨˜æ†¶é«”ç‰©ä»¶
            if torch.cuda.is_available():
                torch.cuda.empty_cache() # å¼·åˆ¶ PyTorch é‡‹æ”¾ VRAM
            print("ğŸ§¹ VRAM cleaned.")
    # --- Step 3: é‚è¼¯å°é½Š ---
    def run_alignment(self, whisper_json, diar_json, final_output_path, chunk_offset_sec=0):
        print(f"ğŸ”— [Step 3] Aligning text with speakers...")
        
        if not os.path.exists(whisper_json) or not os.path.exists(diar_json):
            print("âŒ Input JSONs missing. Step 2 failed.")
            return

        with open(whisper_json, 'r', encoding='utf-8') as f:
            w_segs = json.load(f)
        with open(diar_json, 'r', encoding='utf-8') as f:
            d_segs = json.load(f)
            
        aligned_data = []
        for idx, w in enumerate(w_segs):
            w_start = w["start"]
            w_end = w["end"]
            
            speaker_scores = {}
            for d in d_segs:
                inter_start = max(w_start, d["start"])
                inter_end = min(w_end, d["end"])
                
                if inter_end > inter_start:
                    duration = inter_end - inter_start
                    spk = d["speaker"]
                    speaker_scores[spk] = speaker_scores.get(spk, 0) + duration
            
            if speaker_scores:
                best_speaker = max(speaker_scores, key=speaker_scores.get)
            else:
                best_speaker = "Unknown"

            aligned_data.append({
                "id": f"chunk_{int(chunk_offset_sec)}_{idx}",
                "start": round(w_start + chunk_offset_sec, 2),
                "end": round(w_end + chunk_offset_sec, 2),
                "speaker": best_speaker,
                "text": w["text"].strip(),
                "flag": "review_needed" if best_speaker == "Unknown" else "auto"
            })
            
        with open(final_output_path, 'w', encoding='utf-8') as f:
            json.dump(aligned_data, f, ensure_ascii=False, indent=2)
            
        print(f"ğŸ‰ Final aligned data saved to {final_output_path}")

# --- åŸ·è¡Œå€å¡Š ---
if __name__ == "__main__":
    # ==========================================
    # 1. è¨­å®šè¦è™•ç†çš„æ¡ˆä¾‹è³‡æ–™å¤¾
    # ==========================================
    # å¾ç’°å¢ƒè®Šæ•¸è®€å–æ¡ˆä¾‹åç¨±ï¼Œæˆ–ä½¿ç”¨é è¨­å€¼
    case_name = os.getenv("CASE_NAME")
    project_folder = f"data/{case_name}"
    
    # æª¢æŸ¥è³‡æ–™å¤¾æ˜¯å¦å­˜åœ¨
    if not os.path.exists(project_folder):
        print(f"âŒ éŒ¯èª¤: æ‰¾ä¸åˆ°æ¡ˆä¾‹è³‡æ–™å¤¾ {project_folder}")
        print("è«‹ç¢ºèªæ¡ˆä¾‹åç¨±æ˜¯å¦æ­£ç¢ºï¼Œæˆ–è¨­å®š CASE_NAME ç’°å¢ƒè®Šæ•¸")
        exit(1)
    
    print(f"ğŸš€ Initialize AI Models...")
    print(f"ğŸ“‚ Target folder: {project_folder}")
    
    # æ³¨æ„ï¼šæ¨¡å‹åœ¨æ­¤åˆå§‹åŒ–ï¼Œé¿å…è¿´åœˆå…§é‡è¤‡è¼‰å…¥ï¼Œç¯€çœå¤§é‡æ™‚é–“èˆ‡è¨˜æ†¶é«”
    processor = PipelinePhase2()

    # ==========================================
    # 2. è‡ªå‹•æƒæè©²è³‡æ–™å¤¾ä¸‹çš„æ‰€æœ‰ Chunk wav æª”
    # ==========================================
    search_pattern = os.path.join(project_folder, "chunk_*.wav")
    wav_files = glob.glob(search_pattern)
    
    if not wav_files:
        print(f"âš ï¸ è­¦å‘Š: åœ¨ {project_folder} ä¸­æ‰¾ä¸åˆ°ä»»ä½• chunk_*.wav æª”æ¡ˆ")
        print("è«‹ç¢ºèªæ˜¯å¦å·²ç¶“åŸ·è¡ŒééŸ³è¨Šåˆ‡åˆ†æ­¥é©Ÿ")
        exit(1)
    
    # æ’åºæª”æ¡ˆ (è®“è™•ç†é †åºä¾ç…§ chunk_1, chunk_2... é€²è¡Œ)
    # é€™è£¡ç”¨äº†ä¸€å€‹å°æŠ€å·§ï¼šä¾ç…§æª”åä¸­çš„æ•¸å­—æ’åºï¼Œé¿å… 1, 10, 2 çš„é †åºå•é¡Œ
    try:
        wav_files.sort(key=lambda x: int(os.path.basename(x).split('_')[1]))
    except:
        wav_files.sort() # å¦‚æœæª”åæ ¼å¼ä¸æ¨™æº–ï¼Œå°±ç”¨æ™®é€šæ’åº

    print(f"ğŸ“‚ Found {len(wav_files)} chunks in: {project_folder}")
    for wav_file in wav_files:
        print(f"   - {os.path.basename(wav_file)}")
    print("=========================================")

    # ==========================================
    # 3. æ‰¹æ¬¡åŸ·è¡Œ Pipeline
    # ==========================================
    success_count = 0
    error_count = 0
    
    for target_wav in wav_files:
        filename = os.path.basename(target_wav)
        print(f"\nğŸ”„ Processing: {filename}")
        
        # æº–å‚™è¼¸å‡ºçš„ JSON æª”å (å…¨éƒ¨æ”¾åœ¨åŒä¸€å±¤è³‡æ–™å¤¾)
        base_name_path = os.path.splitext(target_wav)[0]
        json_whisper = f"{base_name_path}_whisper.json"
        json_diar = f"{base_name_path}_diar.json"
        json_final = f"{base_name_path}_aligned.json"
        
        # ---------------------------------------
        # è§£æ Offset (æ™‚é–“åç§»é‡)
        # ---------------------------------------
        # æª”åæ ¼å¼å‡è¨­: chunk_{index}_{start_ms}_{end_ms}.wav
        # ä¾‹å¦‚: chunk_2_531989_1100278.wav -> start_ms = 531989
        try:
            # å»é™¤å‰¯æª”å -> chunk_2_531989_1100278
            # split('_') -> ['chunk', '2', '531989', '1100278']
            # å–å€’æ•¸ç¬¬äºŒå€‹ [-2] -> 531989
            parts = os.path.splitext(filename)[0].split('_')
            if len(parts) >= 4:  # ç¢ºä¿æœ‰è¶³å¤ çš„éƒ¨åˆ†
                start_ms = int(parts[-2])
                offset_sec = start_ms / 1000.0
                print(f"   â±ï¸ Offset detected: {offset_sec}s (Start: {start_ms}ms)")
            else:
                print(f"   âš ï¸ Warning: Unexpected filename format, using offset 0")
                offset_sec = 0.0
        except Exception as e:
            print(f"   âš ï¸ Warning: Could not parse time from filename, default offset to 0. ({e})")
            offset_sec = 0.0

        # ---------------------------------------
        # åŸ·è¡Œ AI è™•ç†
        # ---------------------------------------
        try:
            # 1. Whisper è½‰éŒ„
            if not os.path.exists(json_whisper):
                processor.run_whisper(target_wav, json_whisper)
            else:
                print("   â­ï¸ Whisper output exists, skipping...")

            # 2. Pyannote èªªè©±è€…åˆ†ç†
            if not os.path.exists(json_diar):
                processor.run_diarization(target_wav, json_diar)
            else:
                print("   â­ï¸ Diarization output exists, skipping...")

            # 3. å¼·åˆ¶åŸ·è¡Œ Alignment (å› ç‚ºé€™æ­¥æœ€å¿«ï¼Œä¸”é€šå¸¸éœ€è¦é‡æ–°è¨ˆç®— offset)
            processor.run_alignment(json_whisper, json_diar, json_final, offset_sec)
            
            print(f"   âœ… Done: {filename}")
            success_count += 1
            
        except Exception as e:
            print(f"   âŒ Error processing {filename}: {e}")
            import traceback
            traceback.print_exc()
            error_count += 1

    print(f"\nğŸ‰ Processing completed!")
    print(f"   âœ… Success: {success_count} files")
    print(f"   âŒ Errors: {error_count} files")