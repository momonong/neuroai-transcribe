import os
import json
import torch
import pathlib
import warnings
import gc
from dotenv import load_dotenv

# å¼•å…¥é…ç½®
from core.config import config

# --- 1. è¨­å®šç’°å¢ƒ ---
load_dotenv()
MODEL_ROOT = config.model_cache_dir
os.environ["HF_HOME"] = MODEL_ROOT
os.environ["HF_HUB_DISABLE_SYMLINKS_WARNING"] = "1"

# --- 2. ğŸ›¡ï¸ RTX 5090 / PyTorch 2.8+ çµ‚æ¥µç™½åå–®è£œä¸ ---

print(f"ğŸ”§ Applying PyTorch {torch.__version__} security patches...")

try:
    # åŒ¯å…¥ Pyannote ä»»å‹™å®šç¾©æ¨¡çµ„
    import pyannote.audio.core.task
    from torch.torch_version import TorchVersion
    
    # å®šç¾©æˆ‘å€‘éœ€è¦è§£é–çš„é¡åˆ¥åç¨±æ¸…å–® (é€™æ˜¯ Pyannote æ¨¡å‹çš„æ ¸å¿ƒä¸‰å·¨é ­)
    target_classes = ["Specifications", "Problem", "Resolution"]
    
    safe_list = [TorchVersion, pathlib.PosixPath, pathlib.WindowsPath]
    
    # å‹•æ…‹æŠ“å– Pyannote çš„é¡åˆ¥
    for name in target_classes:
        if hasattr(pyannote.audio.core.task, name):
            cls = getattr(pyannote.audio.core.task, name)
            safe_list.append(cls)
            print(f"   -> Found and added to safelist: {name}")
    
    # è¨»å†Šç™½åå–®
    if hasattr(torch.serialization, "add_safe_globals"):
        torch.serialization.add_safe_globals(safe_list)
        print("âœ… Safe globals registered successfully.")
        
except ImportError as e:
    print(f"âš ï¸ Patch warning: Could not import pyannote modules ({e})")
except Exception as e:
    print(f"âš ï¸ Patch warning: {e}")

# å†æ¬¡å¼·åˆ¶ Patch torch.load (é›™é‡ä¿éšª)
original_load = torch.load
def permissive_load(*args, **kwargs):
    if 'weights_only' not in kwargs:
        kwargs['weights_only'] = False
    return original_load(*args, **kwargs)
torch.load = permissive_load

from typing import Optional
# --- 3. åŒ¯å…¥é‡å‹å¥—ä»¶ ---
from faster_whisper import WhisperModel
from pyannote.audio import Pipeline

class PipelinePhase2:
    def __init__(self, device: Optional[str] = None):
        self.device = device or config.device
        self.compute_type = config.compute_type if torch.cuda.is_available() else "int8"
        
        if not config.hf_token:
            print("âš ï¸ è­¦å‘Š: æœªåµæ¸¬åˆ° HF_TOKENï¼ŒPyannote å¯èƒ½æœƒå ±éŒ¯ã€‚")

    # --- Step 1: Whisper ---
    def run_whisper(self, audio_path, output_json_path):
        if os.path.exists(output_json_path):
            print(f"â© Whisper output exists, skipping.")
            return

        print(f"ğŸ§ [Step 1] Running Whisper on {os.path.basename(audio_path)}...")
        model = WhisperModel(
            config.whisper_model, 
            device=self.device, 
            compute_type=self.compute_type,
            download_root=MODEL_ROOT 
        )

        segments, info = model.transcribe(
            audio_path,
            beam_size=config.whisper_beam_size,
            word_timestamps=True,
            vad_filter=True,
            language=config.whisper_language
        )
        
        results = []
        for seg in segments:
            results.append({
                "start": seg.start,
                "end": seg.end,
                "text": seg.text,
                "words": [{"start": w.start, "end": w.end, "word": w.word} for w in seg.words]
            })
            
        with open(output_json_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… Whisper done. Saved to {output_json_path}")

        # å¾¹åº•é‡‹æ”¾è¨˜æ†¶é«”çš„ä¸‰é€£æ“Š
        del model
        gc.collect() # å¼·åˆ¶ Python å›æ”¶è¨˜æ†¶é«”ç‰©ä»¶
        torch.cuda.empty_cache() # å¼·åˆ¶ PyTorch é‡‹æ”¾ VRAM
        print("ğŸ§¹ VRAM cleaned.")

    # --- Step 2: Pyannote ---
    def run_diarization(self, audio_path, output_json_path):
        if os.path.exists(output_json_path):
            print(f"â© Diarization output exists, skipping.")
            return

        print(f"ğŸ—£ï¸ [Step 2] Running Pyannote on {os.path.basename(audio_path)}...")
        
        hf_token = config.hf_token
        try:
            pipeline = Pipeline.from_pretrained(
                "pyannote/speaker-diarization-3.1",
                use_auth_token=hf_token,
                cache_dir=MODEL_ROOT
            ).to(torch.device(self.device))
            
            diarization = pipeline(audio_path)

        except Exception as e:
            print(f"âŒ Pyannote loading failed: {e}")
            # å¦‚æœé‚„ç¼ºä»€éº¼ï¼Œé¡¯ç¤ºå‡ºä¾†æ–¹ä¾¿é™¤éŒ¯
            import traceback
            traceback.print_exc()
            return

        diar_segments = []
        for turn, _, speaker in diarization.itertracks(yield_label=True):
            diar_segments.append({
                "start": turn.start,
                "end": turn.end,
                "speaker": speaker
            })
            
        with open(output_json_path, 'w', encoding='utf-8') as f:
            json.dump(diar_segments, f, ensure_ascii=False, indent=2)

        print(f"âœ… Diarization done. Saved to {output_json_path}")

        # å¾¹åº•é‡‹æ”¾è¨˜æ†¶é«”çš„ä¸‰é€£æ“Š
        del pipeline
        gc.collect() # å¼·åˆ¶ Python å›æ”¶è¨˜æ†¶é«”ç‰©ä»¶
        torch.cuda.empty_cache() # å¼·åˆ¶ PyTorch é‡‹æ”¾ VRAM
        print("ğŸ§¹ VRAM cleaned.")
    # --- Step 3: é‚è¼¯å°é½Š ---
    def run_alignment(self, whisper_json, diar_json, final_output_path, chunk_offset_sec=0):
        print(f"ğŸ”— [Step 3] Aligning text with speakers...")
        
        if not os.path.exists(whisper_json) or not os.path.exists(diar_json):
            print("âŒ Input JSONs missing. Step 2 failed.")
            return

        with open(whisper_json, 'r', encoding='utf-8') as f:
            w_segs = json.load(f)
        with open(diar_json, 'r', encoding='utf-8') as f:
            d_segs = json.load(f)
            
        aligned_data = []
        for idx, w in enumerate(w_segs):
            w_start = w["start"]
            w_end = w["end"]
            
            speaker_scores = {}
            for d in d_segs:
                inter_start = max(w_start, d["start"])
                inter_end = min(w_end, d["end"])
                
                if inter_end > inter_start:
                    duration = inter_end - inter_start
                    spk = d["speaker"]
                    speaker_scores[spk] = speaker_scores.get(spk, 0) + duration
            
            if speaker_scores:
                best_speaker = max(speaker_scores, key=speaker_scores.get)
            else:
                best_speaker = "Unknown"

            aligned_data.append({
                "id": f"chunk_{int(chunk_offset_sec)}_{idx}",
                "start": round(w_start + chunk_offset_sec, 2),
                "end": round(w_end + chunk_offset_sec, 2),
                "speaker": best_speaker,
                "text": w["text"].strip(),
                "flag": "review_needed" if best_speaker == "Unknown" else "auto"
            })
            
        with open(final_output_path, 'w', encoding='utf-8') as f:
            json.dump(aligned_data, f, ensure_ascii=False, indent=2)
            
        print(f"ğŸ‰ Final aligned data saved to {final_output_path}")

# --- åŸ·è¡Œå€å¡Š ---
if __name__ == "__main__":
    target_wav = "data/temp_chunks/chunk_4_1606067_2171157.wav" 
    
    base_name = os.path.splitext(target_wav)[0]
    json_whisper = f"{base_name}_whisper.json"
    json_diar = f"{base_name}_diar.json"
    json_final = f"{base_name}_aligned.json"
    
    try:
        start_ms = int(base_name.split('_')[-2]) 
        offset_sec = start_ms / 1000.0
    except:
        offset_sec = 0

    processor = PipelinePhase2()
    
    processor.run_whisper(target_wav, json_whisper)
    processor.run_diarization(target_wav, json_diar)
    processor.run_alignment(json_whisper, json_diar, json_final, offset_sec)