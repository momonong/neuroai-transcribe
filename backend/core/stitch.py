import json
import difflib
import time
from typing import List
from openai import OpenAI, APITimeoutError
import instructor
from pydantic import BaseModel, Field
import os

# å¼•å…¥ä½ çš„ config (ç‚ºäº†è®€å–æ­£ç¢ºçš„ API URL)
from .config import config

# --- Agent è¼¸å‡ºçµæ§‹ ---
class VerifiedSentence(BaseModel):
    text: str = Field(..., description="The cleaned, merged sentence.")
    source_ids: List[str] = Field(..., description="IDs of the raw segments used.")

class DatasetEntry(BaseModel):
    sentences: List[VerifiedSentence]

# --- å¹»è¦ºåµæ¸¬ ---
def check_hallucination(original_text: str, rewritten_text: str) -> float:
    return difflib.SequenceMatcher(None, original_text, rewritten_text).ratio()

# --- åˆå§‹åŒ– ---
# 1. æ”¹å‹•ï¼šåŠ å…¥ timeout è¨­å®šï¼Œä¸¦ä½¿ç”¨ config ä¸­çš„ url
client = OpenAI(
    base_url=config.llm_api_url, 
    api_key=config.openai_api_key,
    timeout=180.0  # <--- é—œéµæ”¹å‹•ï¼šè¨­å®š 180 ç§’è¶…æ™‚ (åŸæœ¬é è¨­é€šå¸¸å¤ªçŸ­)
)
agent = instructor.patch(client, mode=instructor.Mode.JSON)

def process_batch_safe(batch_segments):
    input_text = "\n".join([f"[ID {s['id']}] {s['text']}" for s in batch_segments])

    system_prompt = """
    You are a Data Archivist creating a high-quality dataset for NeuroAI research.
    Your task is to merge fragmented speech segments into complete sentences.
    Rules: Verbatim Accuracy, Minimal Edits, Add Punctuation.
    """
    
    # 2. æ”¹å‹•ï¼šåŠ å…¥é‡è©¦è¿´åœˆ
    max_retries = 3
    for attempt in range(max_retries):
        try:
            resp = agent.chat.completions.create(
                model="gemma-2-9b-it", # è«‹ç¢ºä¿é€™è£¡çš„æ¨¡å‹åç¨±èˆ‡ä½  Local LLM è¼‰å…¥çš„ä¸€è‡´
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": f"Raw Data:\n{input_text}"}
                ],
                response_model=DatasetEntry,
                temperature=0.0,
                max_retries=2 # é€™æ˜¯ instructor å…§éƒ¨çš„é‡è©¦ (é‡å°æ ¼å¼éŒ¯èª¤)
            )
            return resp
            
        except APITimeoutError:
            print(f"   âš ï¸ Timeout (Attempt {attempt+1}/{max_retries}). Retrying...", end="\r", flush=True)
            time.sleep(2) # ä¼‘æ¯ä¸€ä¸‹å†è©¦
        except Exception as e:
            print(f"   âš ï¸ Stitch Agent Error (Attempt {attempt+1}): {e}")
            time.sleep(1)
            
    return None # ä¸‰æ¬¡éƒ½å¤±æ•—æ‰å›å‚³ None

# --- æ ¸å¿ƒå…¥å£å‡½å¼ ---
def run_stitching_logic(raw_data: List[dict]):
    final_results = []
    
    # 3. æ”¹å‹•ï¼šå°‡è¦–çª—ç¸®å°ï¼Œæ¸›å°‘ LLM è² æ“”
    WINDOW_SIZE = 5 # <--- å¾ 10 æ”¹æˆ 5ï¼Œé€™æœƒè®“é€Ÿåº¦è®Šæ…¢ä¸€é»ï¼Œä½†ç©©å®šæ€§å¤§å¹…æå‡
    
    print(f"ğŸ›¡ï¸ Starting Stitching Pipeline (Total: {len(raw_data)} segments)...")
    
    total_batches = (len(raw_data) + WINDOW_SIZE - 1) // WINDOW_SIZE
    
    for i in range(0, len(raw_data), WINDOW_SIZE):
        batch = raw_data[i : i + WINDOW_SIZE]
        batch_map = {seg['id']: seg for seg in batch}
        
        current_batch_num = (i // WINDOW_SIZE) + 1
        print(f"   Processing Batch {current_batch_num}/{total_batches}...", end="", flush=True)

        result = process_batch_safe(batch)
        
        if result:
            print(f" Done. ({len(result.sentences)} sents)", flush=True)
            for sent in result.sentences:
                valid_ids = [bid for bid in sent.source_ids if bid in batch_map]
                if not valid_ids: continue
                
                original_text_combined = "".join([batch_map[bid]['text'] for bid in valid_ids])
                
                similarity = check_hallucination(original_text_combined, sent.text)
                len_ratio = len(sent.text) / (len(original_text_combined) + 1)
                is_risky = (similarity < 0.6) or (len_ratio > 1.5) or (len_ratio < 0.5)

                start_time = batch_map[valid_ids[0]]['start']
                end_time = batch_map[valid_ids[-1]]['end']
                speaker = batch_map[valid_ids[0]]['speaker']

                final_item = {
                    "start": start_time,
                    "end": end_time,
                    "speaker": speaker,
                    "text": sent.text,
                    "source_ids": valid_ids,
                    "verification_score": round(similarity, 2),
                    "status": "Verified",
                    "sentence_id": len(final_results)
                }

                if is_risky:
                    # print(f"\n   ğŸš¨ HALLUCINATION (Score: {similarity:.2f}): {sent.text}")
                    final_item['text'] = original_text_combined
                    final_item['status'] = "Rejected_Raw_Kept"

                final_results.append(final_item)
        else:
            # å¤±æ•—å›é€€æ©Ÿåˆ¶
            print(f" Failed. Using Raw Fallback.", flush=True)
            for item in batch:
                item['verification_score'] = 1.0
                item['status'] = "Raw_Fallback"
                item['sentence_id'] = len(final_results)
                final_item = {
                    "start": item['start'],
                    "end": item['end'],
                    "speaker": item['speaker'],
                    "text": item['text'],
                    "source_ids": [item['id']],
                    "verification_score": 1.0,
                    "status": "Raw_Fallback",
                    "sentence_id": len(final_results)
                }
                final_results.append(final_item)

    print("\nâœ… Stitching complete.")
    return final_results