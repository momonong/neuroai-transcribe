import json
import time
from typing import List, Optional, Literal
from enum import Enum
from pydantic import BaseModel, Field
import instructor
from openai import OpenAI, APITimeoutError, APIConnectionError

# å¼•å…¥é…ç½®ï¼Œç¢ºä¿ API Key å’Œ URL èˆ‡ pipeline ä¸€è‡´
from .config import config

# --- 1. å®šç¾©è³‡æ–™çµæ§‹ ---
class IssueCategory(str, Enum):
    LIKELY_ASR_ERROR = "Likely_ASR_Error"     # è½èµ·ä¾†åƒåŒéŸ³å­—éŒ¯èª¤
    CONTEXT_MISMATCH = "Context_Mismatch"     # ä¸Šä¸‹æ–‡ä¸é€šé †
    UNINTELLIGIBLE = "Unintelligible"         # èªžæ„ä¸æ˜Ž
    # ASD ç‰¹å¾µä¿ç•™ (å¯é¸)
    # ECHOLALIA = "Echolalia" 

class SentenceHealth(BaseModel):
    sentence_id: int
    is_suspicious: bool = Field(..., description="True if text seems wrong/weird.")
    issue_category: Optional[IssueCategory] = Field(None)
    reason: Optional[str] = Field(None)
    
    # ðŸ‘‡ðŸ‘‡ðŸ‘‡ æ ¸å¿ƒæ–°å¢žï¼šå»ºè­°ä¿®æ­£æ¬„ä½ ðŸ‘‡ðŸ‘‡ðŸ‘‡
    suggested_correction: Optional[str] = Field(
        None, 
        description="The corrected text IF it is an ASR error. If it is just weird speech behavior (echolalia), leave this null."
    )

class HealthReport(BaseModel):
    assessments: List[SentenceHealth]

# --- 2. åˆå§‹åŒ– Agent ---
client = OpenAI(
    base_url=config.llm_api_url, 
    api_key=config.openai_api_key, 
    timeout=120.0  # è¨­å®š 120 ç§’ï¼Œé¿å… Local LLM é‹ç®—éŽä¹…å°Žè‡´è¶…æ™‚
)
agent = instructor.patch(client, mode=instructor.Mode.JSON)

def analyze_batch_safe(batch_sentences: List[dict]) -> Optional[HealthReport]:
    """
    å‘¼å« LLM é€²è¡Œåˆ†æžï¼Œå¸¶æœ‰é‡è©¦æ©Ÿåˆ¶
    """
    # æº–å‚™ Prompt ä¸Šä¸‹æ–‡
    context = "\n".join([f"[ID {s.get('sentence_id', i)}] {s['text']}" for i, s in enumerate(batch_sentences)])
    
    system_prompt = """
        ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„ä¸­æ–‡é€å­—ç¨¿æ ¡å°å“¡ (Transcription Proofreader)ã€‚
        ä½ çš„ä»»å‹™æ˜¯æª¢æŸ¥èªžéŸ³è¾¨è­˜ (ASR) çš„çµæžœï¼Œä¸¦æ¨™è¨˜å‡ºæ˜Žé¡¯çš„ã€ŒåŒéŸ³éŒ¯å­—ã€æˆ–ã€Œèªžæ„ä¸é€šã€çš„éŒ¯èª¤ã€‚

        è«‹åš´æ ¼éµå®ˆä»¥ä¸‹è¦å‰‡ï¼š
        1. **ä¿ç•™é‡è¤‡èˆ‡çµå·´**ï¼šé€™ä»½è³‡æ–™åŒ…å«è‡ªé–‰ç—‡å…’ç«¥çš„å°è©±ï¼Œå› æ­¤ã€Œé‡è¤‡èªžå¥ã€(å¦‚ï¼šæˆ‘è¦...æˆ‘è¦...æˆ‘è¦) æˆ–ã€Œç„¡æ„ç¾©çš„è²éŸ³ã€æ˜¯é‡è¦çš„è¡Œç‚ºç‰¹å¾µï¼Œ**çµ•å°ä¸è¦**è¦–ç‚ºéŒ¯èª¤ï¼Œä¹Ÿä¸è¦ä¿®æ­£ã€‚
        2. **åƒ…ä¿®æ­£æ˜Žé¡¯éŒ¯å­—**ï¼šåªä¿®æ­£é‚£äº›è®€éŸ³ç›¸è¿‘ä½†å­—éŒ¯èª¤çš„æƒ…æ³ (ä¾‹å¦‚ï¼šã€Œå¤©æ°£å¾ˆè—ã€-> æ‡‰ç‚ºã€Œå¤©æ°£å¾ˆé›£ã€ æˆ– ã€Œæˆ‘æƒ³åŽ»å¹¹å˜›ã€->ã€Œæˆ‘æƒ³åŽ»çŽ©ã€)ã€‚
        3. **ç¹é«”ä¸­æ–‡è¼¸å‡º**ï¼šæ‰€æœ‰ä¿®æ­£å¾Œçš„å»ºè­°å¿…é ˆä½¿ç”¨ã€Œè‡ºç£ç¹é«”ä¸­æ–‡ã€ã€‚

        è¼¸å‡ºæ ¼å¼ (JSON)ï¼š
        {
            "is_suspicious": true/false,
            "suggested_correction": "ä¿®æ­£å¾Œçš„å¥å­" (è‹¥ç„¡éŒ¯èª¤å‰‡å¡« null),
            "reason": "ç°¡çŸ­èªªæ˜Žä¿®æ­£åŽŸå› "
        }
    """

    # 3. é¡¯å¼é‡è©¦è¿´åœˆ
    max_retries = 3
    for attempt in range(max_retries):
        try:
            resp = agent.chat.completions.create(
                model="gemma-2-9b-it", # ç¢ºèªä½ çš„æ¨¡åž‹åç¨±
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": context}
                ],
                response_model=HealthReport,
                temperature=0.1,
                max_retries=2 
            )
            return resp
            
        except (APITimeoutError, APIConnectionError) as e:
            print(f"      âš ï¸ Timeout (Attempt {attempt+1}/{max_retries}). Retrying...", end="\r", flush=True)
            time.sleep(2)
        except Exception as e:
            print(f"      âš ï¸ Flag Agent Error (Attempt {attempt+1}): {e}")
            time.sleep(1)
            
    return None

# --- 4. æ ¸å¿ƒå…¥å£ ---
def run_anomaly_detector(data: List[dict]) -> List[dict]:
    print(f"ðŸ›¡ï¸ Starting Anomaly/QA Detection (Total: {len(data)} sentences)...")
    
    # åˆå§‹åŒ–æ¬„ä½
    for idx, item in enumerate(data):
        if 'sentence_id' not in item:
            item['sentence_id'] = idx
        item['needs_review'] = False
        item['review_reason'] = None
        item['suggested_correction'] = None # åˆå§‹åŒ–å»ºè­°æ¬„ä½

    # è¨­å®š Batch Size ç‚º 5 (ç©©å®šæ€§å„ªå…ˆ)
    batch_size = 5 
    total_batches = (len(data) + batch_size - 1) // batch_size

    for i in range(0, len(data), batch_size):
        batch = data[i : i + batch_size]
        
        current_batch = (i // batch_size) + 1
        print(f"   Processing Batch {current_batch}/{total_batches}...", end="", flush=True)
        
        report = analyze_batch_safe(batch)
        
        if report:
            print(f" Done.", flush=True)
            # å»ºç«‹æŸ¥è¡¨å­—å…¸
            assessment_map = {a.sentence_id: a for a in report.assessments}
            
            for item in batch:
                sid = item['sentence_id']
                if sid in assessment_map:
                    assessment = assessment_map[sid]
                    
                    # åªæœ‰çœŸçš„æœ‰å•é¡Œæ™‚æ‰æ¨™è¨˜
                    if assessment.is_suspicious:
                        item['needs_review'] = True
                        item['review_reason'] = f"[{assessment.issue_category}] {assessment.reason}"
                        # å„²å­˜å»ºè­°ä¿®æ­£
                        item['suggested_correction'] = assessment.suggested_correction
                        
                        # (å¯é¸) Debug é¡¯ç¤º
                        # print(f"\n      ðŸš© Flagged: {item['text']} -> Suggest: {assessment.suggested_correction}")
        else:
            print(f" Failed. Skipping flags.", flush=True)
            for item in batch:
                item['review_reason'] = "Analysis_Failed"

    print("\nâœ… Anomaly Detector Finished.")
    return data