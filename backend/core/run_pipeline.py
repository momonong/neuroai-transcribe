import os
import gc
import json
import torch
import sys
from pathlib import Path
from typing import Optional, List, Dict, Any

# å°‡ backend ç›®éŒ„åŠ å…¥ sys.path ä»¥è§£æ±º core æ¨¡çµ„å°å…¥å•é¡Œ
backend_dir = str(Path(__file__).resolve().parent.parent)
if backend_dir not in sys.path:
    sys.path.insert(0, backend_dir)

# å¼•å…¥æ ¸å¿ƒæ¨¡çµ„
from core.config import config
from core.file_manager import file_manager

# å¼•å…¥åŠŸèƒ½æ¨¡çµ„
from core.split import SmartAudioSplitter
from core.pipeline import PipelinePhase2
from core.stitch import run_stitching_logic
from core.flag import run_anomaly_detector

class NeuroAIPipeline:
    """
    NeuroAI è‡ªå‹•åŒ–è½‰éŒ„æµç¨‹æ§åˆ¶å™¨
    è² è²¬ä¸²æ¥ï¼šåˆ‡åˆ† -> è¾¨è­˜(Whisper+Diarization) -> åˆä½µ -> ç•°å¸¸æ¨™è¨˜
    """

    def __init__(self):
        self.processor: Optional[PipelinePhase2] = None

    def run(self, video_path: str, case_name: Optional[str] = None) -> Optional[str]:
        """
        åŸ·è¡Œå®Œæ•´ Pipeline
        """
        # 1. åˆå§‹åŒ–æ¡ˆä¾‹
        case_name = file_manager.create_case(video_path, case_name)
        
        print(f"ğŸš€ [Pipeline] Start: {case_name}")
        print(f"ğŸ“‚ [Path] Source: {file_manager.get_source_dir(case_name)}")

        try:
            # --- Phase 1: åˆ‡åˆ† ---
            chunk_metadata = self._step_1_split(video_path, case_name)
            if not chunk_metadata:
                raise ValueError("Splitting failed, no chunks generated.")

            # --- Phase 2: è¾¨è­˜èˆ‡å°é½Š ---
            aligned_segments = self._step_2_process(chunk_metadata, case_name)
            
            # --- Phase 3: å¥å­ä¿®å¾© (åŠ å…¥æ–·é»çºŒå‚³) ---
            # ä¿®æ”¹é»ï¼šå‚³å…¥ case_name ä»¥ä¾¿æª¢æŸ¥æª”æ¡ˆæ˜¯å¦å­˜åœ¨
            stitched_data = self._step_3_stitch(aligned_segments, case_name)
            
            # å„²å­˜ä¸­é–“ç”¢ç‰© (åˆ† Chunk)
            # å³ä½¿æ˜¯è®€å–å¿«å–ï¼Œé€™è£¡é‡è·‘ä¸€æ¬¡å­˜æª”ä¹Ÿæ²’é—œä¿‚ (å¾ˆå¿«)ï¼Œç¢ºä¿æª”æ¡ˆä¸€è‡´æ€§
            if stitched_data:
                self._save_stitched_intermediate(stitched_data, chunk_metadata, case_name)
            
            # --- Phase 4: ç•°å¸¸æ¨™è¨˜ ---
            final_data = self._step_4_flag(stitched_data, case_name) # ä¹Ÿå‚³å…¥ case_name çµ¦ Phase 4 æ“´å……ç”¨

            # --- Final: è¼¸å‡º ---
            output_path = file_manager.get_output_file_path(case_name, "transcript.json")
            file_manager.save_json(final_data, output_path, backup=True)
            
            print(f"\nâœ… [Pipeline] Complete! Output: {output_path}")
            return str(output_path)

        except Exception as e:
            print(f"\nâŒ [Pipeline] Failed: {e}")
            import traceback
            traceback.print_exc()
            return None
        finally:
            self._cleanup_resources()

    # ====================================================
    # å…§éƒ¨æ­¥é©Ÿ (Private Methods)
    # ====================================================

    def _step_1_split(self, video_path: str, case_name: str) -> List[Dict]:
        print("\nâœ‚ï¸ --- Phase 1: Audio Splitting ---")
        inter_dir = file_manager.get_intermediate_dir(case_name)
        splitter = SmartAudioSplitter(output_dir=str(inter_dir))
        chunks = splitter.split_audio(video_path, num_chunks=config.default_num_chunks)
        print(f"   âœ… Split into {len(chunks)} chunks.")
        return chunks

    def _step_2_process(self, chunk_metadata: List[Dict], case_name: str) -> List[Dict]:
        print("\nğŸ¤– --- Phase 2: Batch Processing (Whisper -> Diarization) ---")
        
        if self.processor is None:
            self.processor = PipelinePhase2()

        inter_dir = file_manager.get_intermediate_dir(case_name)
        
        whisper_tasks = []
        diar_tasks = []
        alignment_tasks = []

        for meta in chunk_metadata:
            wav_path = meta['file_path']
            base_name = os.path.splitext(os.path.basename(wav_path))[0]
            offset_sec = meta['start_time_ms'] / 1000.0
            
            j_w = inter_dir / f"{base_name}_whisper.json"
            j_d = inter_dir / f"{base_name}_diar.json"
            j_a = inter_dir / f"{base_name}_aligned.json"
            
            whisper_tasks.append({'wav': str(wav_path), 'json': str(j_w)})
            diar_tasks.append({'wav': str(wav_path), 'json': str(j_d)})
            alignment_tasks.append({'w': str(j_w), 'd': str(j_d), 'out': str(j_a), 'offset': offset_sec})

        self.processor.run_whisper_batch(whisper_tasks)
        self.processor.run_diarization_batch(diar_tasks)
        
        print(f"\nğŸ”— [Batch Alignment] processing...", flush=True)
        all_segments = []
        for task in alignment_tasks:
            self.processor.run_alignment(task['w'], task['d'], task['out'], chunk_offset_sec=task['offset'])
            out_path = Path(task['out'])
            if out_path.exists():
                segs = file_manager.load_json(out_path)
                if segs: all_segments.extend(segs)

        raw_path = file_manager.get_output_file_path(case_name, "raw_aligned_transcript.json")
        all_segments.sort(key=lambda x: x.get('start', 0))
        file_manager.save_json(all_segments, raw_path, backup=False)
        
        return all_segments

    # ğŸ‘‡ğŸ‘‡ğŸ‘‡ ä¿®æ”¹é‡é»ï¼šåŠ å…¥æª¢æŸ¥é‚è¼¯ ğŸ‘‡ğŸ‘‡ğŸ‘‡
    def _step_3_stitch(self, segments: List[Dict], case_name: str) -> List[Dict]:
        print("\nğŸ”— --- Phase 3: Stitching ---")
        
        # 1. æª¢æŸ¥æ˜¯å¦å­˜åœ¨å·²ä¿®å¾©çš„å®Œæ•´æª”æ¡ˆ
        inter_dir = file_manager.get_intermediate_dir(case_name)
        full_stitched_path = inter_dir / "full_stitched_transcript.json"
        
        if full_stitched_path.exists():
            print(f"   â© Found existing stitched data: {full_stitched_path.name}")
            print(f"   â© Skipping LLM processing.")
            return file_manager.load_json(full_stitched_path)

        # 2. å¦‚æœä¸å­˜åœ¨ï¼Œæ‰åŸ·è¡Œ LLM
        if not segments:
            print("   âš ï¸ No segments to stitch.")
            return []
        
        stitched = run_stitching_logic(segments)
        print(f"   âœ… Stitched {len(segments)} segments into {len(stitched)} sentences.")
        return stitched
    # ğŸ‘†ğŸ‘†ğŸ‘† ä¿®æ”¹çµæŸ ğŸ‘†ğŸ‘†ğŸ‘†

    def _save_stitched_intermediate(self, stitched_data: List[Dict], chunk_metadata: List[Dict], case_name: str):
        print(f"   ğŸ’¾ Saving intermediate stitched files (per chunk)...")
        inter_dir = file_manager.get_intermediate_dir(case_name)
        
        full_path = inter_dir / "full_stitched_transcript.json"
        file_manager.save_json(stitched_data, full_path, backup=False)

        for meta in chunk_metadata:
            wav_path = meta['file_path']
            base_name = os.path.splitext(os.path.basename(wav_path))[0]
            start_sec = meta['start_time_ms'] / 1000.0
            end_sec = meta.get('end_time_ms', float('inf')) / 1000.0
            
            chunk_sentences = [
                s for s in stitched_data 
                if s['start'] >= start_sec and s['start'] < end_sec
            ]
            
            if chunk_sentences:
                chunk_json_path = inter_dir / f"{base_name}_stitched.json"
                file_manager.save_json(chunk_sentences, chunk_json_path, backup=False)

    def _step_4_flag(self, segments: List[Dict], case_name: str) -> List[Dict]:
        print("\nğŸš© --- Phase 4: Anomaly Detection ---")
        
        # é€™è£¡ä¹Ÿå¯ä»¥è€ƒæ…®åŠ æ–·é»çºŒå‚³ï¼Œçœ‹ä½ çš„éœ€æ±‚
        # å¦‚æœ output/transcript.json å·²ç¶“å­˜åœ¨ä¸”å®Œæ•´ï¼Œå…¶å¯¦ä¹Ÿå¯ä»¥è·³é
        # ä½†å› ç‚º Phase 4 é€šå¸¸æ˜¯æœ€å¾Œä¸€æ­¥ï¼Œä¿ç•™é‡è·‘å½ˆæ€§é€šå¸¸æ¯”è¼ƒå¥½
        
        if not segments:
            return []
            
        final_data = run_anomaly_detector(segments)
        flag_count = sum(1 for s in final_data if s.get('flags'))
        print(f"   âœ… Detection complete. Found {flag_count} flagged sentences.")
        return final_data

    def _cleanup_resources(self):
        if self.processor:
            del self.processor
            self.processor = None
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        print("   ğŸ§¹ Resources cleaned up.")

def run_pipeline(video_path: str):
    pipeline = NeuroAIPipeline()
    return pipeline.run(video_path)

if __name__ == "__main__":
    import argparse
    import sys

    parser = argparse.ArgumentParser(description="NeuroAI Pipeline")
    parser.add_argument("video_path", help="è¼¸å…¥å½±ç‰‡æª”æ¡ˆçš„è·¯å¾‘")
    parser.add_argument("--case", help="æŒ‡å®šæ¡ˆä¾‹åç¨± (å¯é¸)", default=None)
    args = parser.parse_args()

    if not os.path.exists(args.video_path):
        print(f"âŒ Error: æ‰¾ä¸åˆ°æª”æ¡ˆ: {args.video_path}")
        sys.exit(1)

    print("=" * 50)
    print(f"ğŸ¬ NeuroAI Pipeline å•Ÿå‹•")
    print(f"ğŸ“„ ç›®æ¨™å½±ç‰‡: {args.video_path}")
    print("=" * 50)

    result = run_pipeline(args.video_path)

    if result:
        print("=" * 50)
        print(f"âœ… è™•ç†æˆåŠŸï¼")
        print(f"ğŸ“‚ è¼¸å‡ºæª”æ¡ˆ: {result}")
        print("=" * 50)
    else:
        print("=" * 50)
        print(f"âŒ è™•ç†å¤±æ•—")
        print("=" * 50)
        sys.exit(1)