import os
import gc
import json
import torch
import sys
from pathlib import Path
from typing import Optional, List, Dict, Any

# å°‡ backend ç›®éŒ„åŠ å…¥ sys.path
backend_dir = str(Path(__file__).resolve().parent.parent)
if backend_dir not in sys.path:
    sys.path.insert(0, backend_dir)

# å¼•å…¥æ ¸å¿ƒæ¨¡çµ„
from core.config import config
from core.file_manager import file_manager

# å¼•å…¥åŠŸèƒ½æ¨¡çµ„
from core.split import SmartAudioSplitter
from core.pipeline import PipelinePhase2
from core.stitch import run_stitching_logic
from core.flag import run_anomaly_detector

class NeuroAIPipeline:
    def __init__(self):
        self.processor: Optional[PipelinePhase2] = None

    def run(self, video_path: str, case_name: Optional[str] = None) -> Optional[str]:
        # 1. åˆå§‹åŒ–æ¡ˆä¾‹
        case_name = file_manager.create_case(video_path, case_name)
        
        print(f"ğŸš€ [Pipeline] Start: {case_name}")
        print(f"ğŸ“‚ [Path] Source: {file_manager.get_source_dir(case_name)}")

        try:
            # --- Phase 1: åˆ‡åˆ† ---
            chunk_metadata = self._step_1_split(video_path, case_name)
            if not chunk_metadata:
                raise ValueError("Splitting failed, no chunks generated.")

            # --- Phase 2: è¾¨è­˜èˆ‡å°é½Š ---
            self._step_2_process(chunk_metadata, case_name)
            
            # --- Phase 3 & 4: åˆ†æ®µä¿®å¾©èˆ‡æ¨™è¨˜ (æ•´åˆè¿´åœˆ) ---
            # é€™æ˜¯æœ€é—œéµçš„ä¿®æ”¹ï¼šæˆ‘å€‘æŠŠ Stitch å’Œ Flag æ•´åˆåœ¨ä¸€å€‹è¿´åœˆè£¡è™•ç†
            # é€™æ¨£æ¯å€‹ Chunk éƒ½æ˜¯ç¨ç«‹çš„ï¼šAligned -> Stitched -> Flagged
            final_data = self._step_3_4_process_per_chunk(chunk_metadata, case_name)

            # --- Final: è¼¸å‡º ---
            # å°‡æ‰€æœ‰ Chunk çš„çµæœåˆä½µå­˜æˆæœ€çµ‚çš„ transcript.json
            output_path = file_manager.get_output_file_path(case_name, "transcript.json")
            file_manager.save_json(final_data, output_path, backup=True)
            
            print(f"\nâœ… [Pipeline] Complete! Output: {output_path}")
            return str(output_path)

        except Exception as e:
            print(f"\nâŒ [Pipeline] Failed: {e}")
            import traceback
            traceback.print_exc()
            return None
        finally:
            self._cleanup_resources()

    # ... (Phase 1 å’Œ Phase 2 çš„ç¨‹å¼ç¢¼ä¿æŒä¸è®Š) ...
    def _step_1_split(self, video_path: str, case_name: str) -> List[Dict]:
        print("\nâœ‚ï¸ --- Phase 1: Audio Splitting ---")
        inter_dir = file_manager.get_intermediate_dir(case_name)
        splitter = SmartAudioSplitter(output_dir=str(inter_dir))
        chunks = splitter.split_audio(video_path, num_chunks=config.default_num_chunks)
        print(f"   âœ… Split into {len(chunks)} chunks.")
        return chunks

    def _step_2_process(self, chunk_metadata: List[Dict], case_name: str) -> List[Dict]:
        print("\nğŸ¤– --- Phase 2: Batch Processing (Whisper -> Diarization) ---")
        if self.processor is None:
            self.processor = PipelinePhase2()
        inter_dir = file_manager.get_intermediate_dir(case_name)
        whisper_tasks = []
        diar_tasks = []
        alignment_tasks = []
        for meta in chunk_metadata:
            wav_path = meta['file_path']
            base_name = os.path.splitext(os.path.basename(wav_path))[0]
            offset_sec = meta['start_time_ms'] / 1000.0
            j_w = inter_dir / f"{base_name}_whisper.json"
            j_d = inter_dir / f"{base_name}_diar.json"
            j_a = inter_dir / f"{base_name}_aligned.json"
            whisper_tasks.append({'wav': str(wav_path), 'json': str(j_w)})
            diar_tasks.append({'wav': str(wav_path), 'json': str(j_d)})
            alignment_tasks.append({'w': str(j_w), 'd': str(j_d), 'out': str(j_a), 'offset': offset_sec})
        self.processor.run_whisper_batch(whisper_tasks)
        self.processor.run_diarization_batch(diar_tasks)
        print(f"\nğŸ”— [Batch Alignment] processing...", flush=True)
        all_segments = []
        for task in alignment_tasks:
            self.processor.run_alignment(task['w'], task['d'], task['out'], chunk_offset_sec=task['offset'])
            out_path = Path(task['out'])
            if out_path.exists():
                segs = file_manager.load_json(out_path)
                if segs: all_segments.extend(segs)
        raw_path = file_manager.get_output_file_path(case_name, "raw_aligned_transcript.json")
        all_segments.sort(key=lambda x: x.get('start', 0))
        file_manager.save_json(all_segments, raw_path, backup=False)
        return all_segments

    # ğŸ‘‡ğŸ‘‡ğŸ‘‡ æ ¸å¿ƒä¿®æ”¹ï¼šæ•´åˆ Phase 3 & 4 ç‚º Per-Chunk è™•ç† ğŸ‘‡ğŸ‘‡ğŸ‘‡
    def _step_3_4_process_per_chunk(self, chunk_metadata: List[Dict], case_name: str) -> List[Dict]:
        print("\nğŸ§  --- Phase 3 & 4: Intelligent Processing (Stitch -> Flag) ---")
        inter_dir = file_manager.get_intermediate_dir(case_name)
        
        all_final_results = []

        for i, meta in enumerate(chunk_metadata):
            wav_path = meta['file_path']
            base_name = os.path.splitext(os.path.basename(wav_path))[0]
            
            # å®šç¾©æª”æ¡ˆè·¯å¾‘
            aligned_path = inter_dir / f"{base_name}_aligned.json"
            stitched_path = inter_dir / f"{base_name}_stitched.json"
            flagged_path = inter_dir / f"{base_name}_flagged_for_human.json" # é€™æ˜¯æˆ‘å€‘è¦ç”¢ç”Ÿçš„æœ€çµ‚åˆ†æ®µæª”
            
            print(f"   Processing Chunk {i+1}/{len(chunk_metadata)}: {base_name}")

            # 1. æª¢æŸ¥ Flagged æ˜¯å¦å·²å­˜åœ¨ (æœ€çµ‚æ–·é»)
            if flagged_path.exists():
                print(f"      â© Found flagged file. Skipping Chunk.")
                chunk_result = file_manager.load_json(flagged_path)
                all_final_results.extend(chunk_result)
                continue

            # 2. æº–å‚™ Stitched è³‡æ–™
            stitched_data = []
            if stitched_path.exists():
                # å¦‚æœæœ‰ stitched å­˜æª”ï¼Œç›´æ¥è®€å–
                print(f"      â© Found stitched file. Loading...")
                stitched_data = file_manager.load_json(stitched_path)
            else:
                # å¦‚æœæ²’æœ‰ï¼ŒåŸ·è¡Œ Stitching
                if not aligned_path.exists():
                    print(f"      âš ï¸ Aligned file missing. Skipping.")
                    continue
                
                raw_segments = file_manager.load_json(aligned_path)
                if not raw_segments: continue

                try:
                    print(f"      ğŸ§µ Stitching...")
                    stitched_data = run_stitching_logic(raw_segments)
                    file_manager.save_json(stitched_data, stitched_path, backup=False)
                except Exception as e:
                    print(f"      âŒ Stitching Failed: {e}")
                    # Fallback to raw
                    stitched_data = raw_segments

            # 3. åŸ·è¡Œ Flagging (Anomaly Detection)
            if stitched_data:
                try:
                    print(f"      ğŸš© Flagging...")
                    flagged_data = run_anomaly_detector(stitched_data)
                    
                    # å­˜æª”ï¼šé€™æ˜¯çµ¦å‰ç«¯è®€å–çš„æœ€çµ‚åˆ†æ®µæª”
                    file_manager.save_json(flagged_data, flagged_path, backup=False)
                    print(f"      ğŸ’¾ Saved: {flagged_path.name}")
                    
                    all_final_results.extend(flagged_data)
                except Exception as e:
                    print(f"      âŒ Flagging Failed: {e}")
                    # Fallback to stitched data (without flags)
                    all_final_results.extend(stitched_data)

        print(f"   âœ… All chunks processed. Total sentences: {len(all_final_results)}")
        return all_final_results
    # ğŸ‘†ğŸ‘†ğŸ‘† ä¿®æ”¹çµæŸ ğŸ‘†ğŸ‘†ğŸ‘†

    def _cleanup_resources(self):
        if self.processor:
            del self.processor
            self.processor = None
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        print("   ğŸ§¹ Resources cleaned up.")

def run_pipeline(video_path: str):
    pipeline = NeuroAIPipeline()
    return pipeline.run(video_path)

# ... (main block ä¿æŒä¸è®Š) ...
if __name__ == "__main__":
    import argparse
    import sys

    parser = argparse.ArgumentParser(description="NeuroAI Pipeline")
    parser.add_argument("video_path", help="è¼¸å…¥å½±ç‰‡æª”æ¡ˆçš„è·¯å¾‘")
    parser.add_argument("--case", help="æŒ‡å®šæ¡ˆä¾‹åç¨± (å¯é¸)", default=None)
    args = parser.parse_args()

    if not os.path.exists(args.video_path):
        print(f"âŒ Error: æ‰¾ä¸åˆ°æª”æ¡ˆ: {args.video_path}")
        sys.exit(1)

    print("=" * 50)
    print(f"ğŸ¬ NeuroAI Pipeline å•Ÿå‹•")
    print(f"ğŸ“„ ç›®æ¨™å½±ç‰‡: {args.video_path}")
    print("=" * 50)

    result = run_pipeline(args.video_path)

    if result:
        print("=" * 50)
        print(f"âœ… è™•ç†æˆåŠŸï¼")
        print(f"ğŸ“‚ è¼¸å‡ºæª”æ¡ˆ: {result}")
        print("=" * 50)
    else:
        print("=" * 50)
        print(f"âŒ è™•ç†å¤±æ•—")
        print("=" * 50)
        sys.exit(1)