import os
import io
import json
import glob
import shutil
from datetime import datetime
from typing import List, Dict, Optional, Any
from pathlib import Path

from fastapi import FastAPI, HTTPException, UploadFile, File, Form, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse
from fastapi.staticfiles import StaticFiles
from pydantic import BaseModel

from core.config import config
from core.file_manager import file_manager
from core.run_pipeline import run_pipeline

app = FastAPI()

# ==========================================
# 1. è¨­å®š & åˆå§‹åŒ– (è·¯å¾‘ä¿®æ­£æ ¸å¿ƒ)
# ==========================================

# å–å¾—ç•¶å‰æª”æ¡ˆ (backend/main.py) çš„çµ•å°è·¯å¾‘
CURRENT_FILE = os.path.abspath(__file__)
# å–å¾— backend è³‡æ–™å¤¾è·¯å¾‘
BACKEND_DIR = os.path.dirname(CURRENT_FILE)
# å–å¾—å°ˆæ¡ˆæ ¹ç›®éŒ„ (backend çš„ä¸Šä¸€å±¤)
PROJECT_ROOT = os.path.dirname(BACKEND_DIR)

# è¨­å®š DATA_DIR ç‚º å°ˆæ¡ˆæ ¹ç›®éŒ„ä¸‹çš„ data
DATA_DIR = os.path.join(PROJECT_ROOT, "data")

# ç¢ºä¿è³‡æ–™å¤¾å­˜åœ¨
os.makedirs(DATA_DIR, exist_ok=True)

print(f"ğŸš€ Server started.")
print(f"ğŸ“‚ Project Root: {PROJECT_ROOT}")
print(f"ğŸ“‚ Data Directory: {DATA_DIR}")

# CORS è¨­å®š
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# æ›è¼‰éœæ…‹æª”æ¡ˆ (å‰ç«¯æ’­æ”¾å½±ç‰‡ç”¨)
# http://localhost:8001/static/CaseName/video.mp4
app.mount("/static", StaticFiles(directory=DATA_DIR), name="static")


# ==========================================
# 2. è³‡æ–™çµæ§‹ (Pydantic Models)
# ==========================================

class TranscriptSegment(BaseModel):
    sentence_id: float
    start: float
    end: float
    speaker: str
    text: str
    verification_score: float = 1.0
    status: str = "reviewed"
    needs_review: bool = False
    review_reason: Optional[str] = None
    suggested_correction: Optional[str] = None # ç¢ºä¿å¾Œç«¯æ¥æ”¶å‰ç«¯å‚³å›çš„è³‡æ–™çµæ§‹å®Œæ•´

class SavePayload(BaseModel):
    filename: str  # ç›¸å°è·¯å¾‘ (CaseName/chunk_x.json)
    speaker_mapping: Dict[str, str]
    segments: List[TranscriptSegment]

# ==========================================
# 3. è¼”åŠ©å‡½å¼
# ==========================================

def get_real_path(relative_path: str):
    """å°‡å‰ç«¯å‚³ä¾†çš„ç›¸å°è·¯å¾‘è½‰æ›ç‚ºç³»çµ±çµ•å°è·¯å¾‘"""
    if ".." in relative_path:
        raise ValueError("Invalid path: '..' is not allowed")
    return os.path.join(DATA_DIR, relative_path)

# ==========================================
# 4. API å¯¦ä½œ
# ==========================================

@app.get("/api/videos")
def get_videos():
    """
    æƒææ‰€æœ‰å½±ç‰‡ï¼Œä¾›å‰ç«¯ä¸‹æ‹‰é¸å–®ä½¿ç”¨
    ä¿®æ­£ï¼šåŒæ™‚æ”¯æ´ data/CaseName/Video.mp4 (èˆŠ) èˆ‡ data/CaseName/source/Video.mp4 (æ–°)
    """
    video_files = []
    # æ”¯æ´å¸¸è¦‹éŸ³è¦–è¨Šæ ¼å¼
    extensions = [".mp4", ".MP4", ".mov", ".MOV", ".avi", ".AVI"]
    
    if not os.path.exists(DATA_DIR):
        return video_files
    
    # æ’é™¤çš„ç³»çµ±è³‡æ–™å¤¾
    IGNORE_DIRS = {"temp_chunks", "db", "text", "__pycache__", "output"}

    # 1. æƒæ data è³‡æ–™å¤¾åº•ä¸‹çš„æ¯ä¸€å±¤ (Caseè³‡æ–™å¤¾)
    with os.scandir(DATA_DIR) as it:
        for entry in it:
            if entry.is_dir() and entry.name not in IGNORE_DIRS:
                case_name = entry.name
                case_path = entry.path
                
                # === æ ¸å¿ƒä¿®æ”¹ï¼šå®šç¾©æœå°‹è·¯å¾‘ (å„ªå…ˆæ‰¾ sourceï¼Œä¹Ÿæ‰¾æ ¹ç›®éŒ„) ===
                search_targets = []
                
                # 1. æ–°æ¶æ§‹: data/Case/source
                source_dir = os.path.join(case_path, "source")
                if os.path.exists(source_dir):
                    search_targets.append(source_dir)
                
                # 2. èˆŠæ¶æ§‹: data/Case
                search_targets.append(case_path)

                # ç”¨ä¾†é¿å…é‡è¤‡ (å¦‚æœåŒä¸€å€‹æª”æ¡ˆè¢«æƒåˆ°å…©æ¬¡)
                seen_files = set()

                for target_dir in search_targets:
                    if not os.path.exists(target_dir): continue

                    for f in os.listdir(target_dir):
                        if any(f.endswith(ext) for ext in extensions):
                            # æ’é™¤æ‰ chunk_ é–‹é ­çš„éŸ³æª”
                            if f.startswith("chunk_"):
                                continue
                            
                            if f in seen_files:
                                continue
                            seen_files.add(f)
                            
                            # å–å¾—çµ•å°è·¯å¾‘
                            full_path = os.path.join(target_dir, f)
                            
                            # è¨ˆç®—ç›¸å°è·¯å¾‘ (çµ¦å‰ç«¯ /static/ ä½¿ç”¨)
                            # å¦‚æœåœ¨ source è£¡ï¼Œrel_path æœƒè®Šæˆ "CaseName/source/video.mp4"
                            # å¦‚æœåœ¨æ ¹ç›®éŒ„ï¼Œrel_path æœƒè®Šæˆ "CaseName/video.mp4"
                            rel_path = os.path.relpath(full_path, DATA_DIR).replace("\\", "/")
                            
                            display_name = f"{case_name}"
                            
                            video_files.append({
                                "path": rel_path,
                                "name": display_name
                            })

    # ä¾åç¨±æ’åº
    video_files.sort(key=lambda x: x['name'], reverse=True)
    return video_files

@app.get("/api/cases")
def get_cases():
    """
    åˆ—å‡º data/ åº•ä¸‹çš„å°ˆæ¡ˆè³‡æ–™å¤¾
    """
    cases = []
    if not os.path.exists(DATA_DIR):
        return cases
    
    # å¿½ç•¥çš„ç³»çµ±è³‡æ–™å¤¾
    IGNORE_DIRS = {"temp_chunks", "db", "text", "__pycache__", "output", "test-complete-pipeline"}

    with os.scandir(DATA_DIR) as it:
        for entry in it:
            if entry.is_dir() and entry.name not in IGNORE_DIRS:
                cases.append(entry.name)
    
    cases.sort(reverse=True)
    return cases

@app.get("/api/temp/chunks")
def list_chunks(case: Optional[str] = None):
    """
    åˆ—å‡º JSON æª”æ¡ˆ (æ™ºæ…§ç¯©é¸ç‰ˆ)ã€‚
    é‚è¼¯ï¼šé‡å°æ¯å€‹ Chunk IDï¼Œåªå›å‚³ã€Œæœ€é«˜å„ªå…ˆç´šã€çš„å–®ä¸€æª”æ¡ˆã€‚
    ä¿®æ­£å¾Œå„ªå…ˆç´š: edited > flagged > verified > aligned
    """
    json_files = []
    
    if case:
        search_path = os.path.join(DATA_DIR, case, "**", "chunk_*.json") # ä½¿ç”¨ recursive glob æ¯”è¼ƒä¿éšªï¼Œä½†ç›®å‰çµæ§‹æ‡‰è©²åªæœ‰ä¸€å±¤
        # ä¿®æ­£ï¼šç›®å‰çš„ intermediate éƒ½åœ¨ data/Case/intermediateï¼Œæ‰€ä»¥æˆ‘å€‘è¦æ‰¾é‚£è£¡
        # æˆ–æ˜¯ä¹‹å‰çš„é‚è¼¯æ˜¯ data/Case/chunk_*.json (èˆŠçµæ§‹)
        # è®“æˆ‘å€‘åŒæ™‚æ”¯æ´å…©è€…
        
        # ç­–ç•¥ï¼šå…ˆæƒ data/Case/intermediate (æ–°çµæ§‹)
        inter_path = os.path.join(DATA_DIR, case, "intermediate", "chunk_*.json")
        files_inter = glob.glob(inter_path)
        
        # å†æƒ data/Case (èˆŠçµæ§‹ï¼Œå¦‚æœæœ‰)
        root_path = os.path.join(DATA_DIR, case, "chunk_*.json")
        files_root = glob.glob(root_path)
        
        all_files = files_inter + files_root
    else:
        # æœå°‹å…¨éƒ¨ (é–‹ç™¼ç”¨)
        all_files = glob.glob(os.path.join(DATA_DIR, "*", "intermediate", "chunk_*.json")) + \
                    glob.glob(os.path.join(DATA_DIR, "*", "chunk_*.json"))
    
    # 1. æ”¶é›†æ‰€æœ‰ chunk æª”æ¡ˆï¼Œä¸¦åˆ†çµ„
    chunk_groups = {}
    
    for f in all_files:
        filename = os.path.basename(f)
        
        # æ’é™¤éç›®æ¨™æª”æ¡ˆ
        if "whisper" in filename or "diar" in filename:
            continue
            
        # è§£æ Chunk ID
        parts = filename.split('_')
        if len(parts) < 2: continue
        
        # å–å¾— Case Name (ç¨å¾®è¤‡é›œå› ç‚ºæœ‰ intermediate å±¤)
        # f = .../data/CaseName/intermediate/chunk... -> dirname -> dirname -> basename
        # f = .../data/CaseName/chunk... -> dirname -> basename
        parent_dir = os.path.dirname(f)
        if os.path.basename(parent_dir) == "intermediate":
             case_name = os.path.basename(os.path.dirname(parent_dir))
        else:
             case_name = os.path.basename(parent_dir)

        chunk_id = f"{parts[0]}_{parts[1]}" # chunk_1
        unique_key = f"{case_name}/{chunk_id}"
        
        if unique_key not in chunk_groups:
            chunk_groups[unique_key] = {}
            
        # åˆ†é¡
        if "flagged_for_human" in filename:
            chunk_groups[unique_key]["flagged"] = f
        elif "edited" in filename:
            chunk_groups[unique_key]["edited"] = f
        elif "verified_dataset" in filename:
            chunk_groups[unique_key]["verified"] = f
        elif "stitched" in filename: # æ–°å¢ï¼šæ”¯æ´æˆ‘å€‘å‰›å‰›åšå‡ºä¾†çš„ stitched æª”æ¡ˆ
            chunk_groups[unique_key]["stitched"] = f
        elif "aligned" in filename:
            chunk_groups[unique_key]["aligned"] = f
            
    # 2. æŒ‘é¸æœ€ä½³æª”æ¡ˆ (Winner Takes All)
    for key, variants in chunk_groups.items():
        best_file = None
        
        # ğŸ”¥ å„ªå…ˆé †åºèª¿æ•´ ğŸ”¥
        if "edited" in variants:
            best_file = variants["edited"]      # ğŸ¥‡ 1. å·²ç·¨è¼¯
        elif "flagged" in variants:
            best_file = variants["flagged"]     # ğŸ¥ˆ 2. éœ€å¯©æ ¸ (AI æ¨™è¨˜)
        elif "stitched" in variants:
             best_file = variants["stitched"]   # ğŸ¥‰ 3. å·²ä¿®å¾© (AI Stitching çµæœ) <--- æ–°å¢
        elif "verified" in variants:
            best_file = variants["verified"]    # 4. èˆŠç‰ˆé©—è­‰
        elif "aligned" in variants:
            best_file = variants["aligned"]     # 5. åŸå§‹æª”
            
        if best_file:
            rel_path = os.path.relpath(best_file, DATA_DIR)
            json_files.append(rel_path.replace("\\", "/"))
            
    # 3. æ’åº (ç¢ºä¿ chunk_1, chunk_2 é †åºæ­£ç¢º)
    def sort_key(path):
        try:
            filename = os.path.basename(path)
            parts = filename.split('_')
            return int(parts[1]) 
        except:
            return 0

    json_files.sort(key=sort_key)
    return {"files": json_files}

@app.get("/api/temp/chunk/{filename:path}")
def get_chunk(filename: str):
    """
    è®€å–å°ˆæ¡ˆè³‡æ–™ (æ™ºæ…§å„ªå…ˆç´šç‰ˆ)ã€‚
    """
    try:
        # 1. å–å¾—çµ•å°è·¯å¾‘
        request_path = get_real_path(filename)
        directory = os.path.dirname(request_path)
        request_fname = os.path.basename(request_path)
        
        # 2. é‚„åŸã€Œæ ¸å¿ƒæª”åã€ (ç§»é™¤æ‰€æœ‰å¯èƒ½çš„å¾Œç¶´)
        core_name = request_fname.replace("_flagged_for_human.json", "")\
                                 .replace("_edited.json", "")\
                                 .replace("_verified_dataset.json", "")\
                                 .replace("_stitched.json", "")\
                                 .replace("_aligned.json", "")\
                                 .replace(".json", "")
        
        # ç§»é™¤å¯èƒ½æ®˜ç•™çš„å¾Œç¶´
        for suffix in ["_whisper", "_aligned", "_diar"]:
            if core_name.endswith(suffix):
                core_name = core_name.replace(suffix, "")

        # 3. å®šç¾©å„ç‰ˆæœ¬çš„å€™é¸è·¯å¾‘
        candidate_edited = os.path.join(directory, f"{core_name}_edited.json")
        candidate_flagged = os.path.join(directory, f"{core_name}_flagged_for_human.json")
        candidate_stitched = os.path.join(directory, f"{core_name}_stitched.json") # æ–°å¢
        candidate_verified = os.path.join(directory, f"{core_name}_verified_dataset.json")
        candidate_aligned = os.path.join(directory, f"{core_name}_aligned.json")
        
        # 4. ä¾ç…§å„ªå…ˆæ¬Šæ±ºå®šæœ€çµ‚è¦è®€å–å“ªå€‹æª”æ¡ˆ
        target_path = None
        
        if os.path.exists(candidate_edited):
            target_path = candidate_edited
            print(f"ğŸ“– Priority Load: Edited ({os.path.basename(target_path)})")
        elif os.path.exists(candidate_flagged):
            target_path = candidate_flagged
            print(f"ğŸ“– Priority Load: Flagged ({os.path.basename(target_path)})")
        elif os.path.exists(candidate_stitched):
            target_path = candidate_stitched
            print(f"ğŸ“– Priority Load: Stitched ({os.path.basename(target_path)})")
        elif os.path.exists(candidate_verified):
            target_path = candidate_verified
        elif os.path.exists(candidate_aligned):
            target_path = candidate_aligned
        else:
            target_path = request_path
            print(f"ğŸ“– Fallback Load: {os.path.basename(target_path)}")

        if not os.path.exists(target_path):
            raise HTTPException(status_code=404, detail="File not found")
        
        # 5. è®€å–æª”æ¡ˆå…§å®¹
        with open(target_path, "r", encoding="utf-8") as f:
            data = json.load(f)

        # ========================================================
        # 6. åª’é«”é…å°é‚è¼¯ (Media Discovery) - æ”¹è‰¯ç‰ˆ
        # ========================================================
        # directory æŒ‡å‘çš„æ˜¯ intermediateï¼Œæˆ‘å€‘éœ€è¦å¾€ä¸Šæ‰¾ source æˆ–æ ¹ç›®éŒ„
        # çµæ§‹ A: data/Case/intermediate/chunk.json -> å½±ç‰‡åœ¨ data/Case/source/
        # çµæ§‹ B: data/Case/chunk.json -> å½±ç‰‡åœ¨ data/Case/
        
        case_root = os.path.dirname(directory) # å‡è¨­ directory æ˜¯ intermediateï¼Œä¸Šä¸€å±¤æ˜¯ Case
        if os.path.basename(directory) != "intermediate":
             case_root = directory # å¦‚æœ json æœ¬ä¾†å°±åœ¨æ ¹ç›®éŒ„
             
        # æœå°‹å€™é¸å½±ç‰‡ç›®éŒ„
        media_search_dirs = [
            os.path.join(case_root, "source"), # å„ªå…ˆæ‰¾ source
            case_root # æ¬¡è¦æ‰¾æ ¹ç›®éŒ„
        ]
        
        target_media = None
        media_folder_found = None
        
        for search_dir in media_search_dirs:
            if not os.path.exists(search_dir): continue
            
            files = os.listdir(search_dir)
            mp4_files = [f for f in files if f.lower().endswith(('.mp4', '.mov', '.avi'))]
            
            if mp4_files:
                mp4_files.sort(key=len) 
                target_media = mp4_files[0]
                media_folder_found = search_dir
                break # æ‰¾åˆ°å°±è·³å‡º
        
        # 7. çµ„è£å›å‚³è³‡æ–™
        processed_data = data if isinstance(data, dict) else {
            "segments": data, 
            "speaker_mapping": {}, 
            "file_type": "original"
        }
        
        if target_media and media_folder_found:
            # è¨ˆç®—ç›¸å°æ–¼ DATA_DIR çš„è·¯å¾‘çµ¦å‰ç«¯
            # ä¾‹å¦‚: Case/source/video.mp4
            full_media_path = os.path.join(media_folder_found, target_media)
            media_rel_path = os.path.relpath(full_media_path, DATA_DIR).replace("\\", "/")
            processed_data['media_file'] = media_rel_path
            
        # æ¨™è¨˜æª”æ¡ˆé¡å‹ (çµ¦å‰ç«¯é¡¯ç¤º Chip ç”¨)
        if "_flagged_for_human" in target_path:
            processed_data['file_type'] = 'flagged'
        elif "_edited" in target_path:
            processed_data['file_type'] = 'edited'
        else:
            processed_data['file_type'] = 'original'
            
        return processed_data

    except Exception as e:
        print(f"âŒ Error loading chunk: {e}")
        raise HTTPException(status_code=500, detail=str(e))
        
@app.post("/api/temp/save")
def save_chunk(payload: SavePayload):
    """
    å­˜æª” APIã€‚
    """
    try:
        # 1. è§£æåŸå§‹è·¯å¾‘
        full_path = get_real_path(payload.filename)
        directory = os.path.dirname(full_path)
        filename = os.path.basename(full_path)
        
        # 2. å»ºæ§‹ç›®æ¨™æª”å (å¼·åˆ¶çµå°¾ç‚º _edited.json)
        core_name = filename.replace("_flagged_for_human.json", "")\
                            .replace("_edited.json", "")\
                            .replace("_aligned.json", "")\
                            .replace("_stitched.json", "")\
                            .replace("_verified_dataset.json", "")\
                            .replace(".json", "")
        
        new_filename = f"{core_name}_edited.json"
        save_path = os.path.join(directory, new_filename)
        
        # 3. æº–å‚™è³‡æ–™
        data_to_save = {
            "last_modified": datetime.now().isoformat(),
            "speaker_mapping": payload.speaker_mapping,
            "segments": [s.dict() for s in payload.segments],
        }
        
        # 4. å¯«å…¥æª”æ¡ˆ
        with open(save_path, "w", encoding="utf-8") as f:
            json.dump(data_to_save, f, ensure_ascii=False, indent=2)
            
        print(f"ğŸ’¾ Saved to: {new_filename}")
        
        # 5. å›å‚³æ–°çš„ç›¸å°è·¯å¾‘
        relative_path = os.path.relpath(save_path, DATA_DIR).replace("\\", "/")
        
        return {
            "status": "success", 
            "saved_to": relative_path,
            "filename": new_filename
        }
    
    except Exception as e:
        print(f"âŒ Save error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/upload")
async def upload_video_endpoint(
    background_tasks: BackgroundTasks,  # â˜… æ³¨å…¥ BackgroundTasks
    file: UploadFile = File(...), 
    case_name: str = Form(...)
):
    try:
        # 1. å„²å­˜åŸå§‹æª”æ¡ˆ
        file_ext = os.path.splitext(file.filename)[1]
        safe_filename = f"{case_name}{file_ext}"
        save_path = os.path.join(DATA_DIR, case_name, "source", safe_filename)
        
        # ç¢ºä¿ç›®éŒ„å­˜åœ¨
        os.makedirs(os.path.dirname(save_path), exist_ok=True)
        
        with open(save_path, "wb") as buffer:
            shutil.copyfileobj(file.file, buffer)
            
        # 2. â˜… é—œéµï¼šä¸è¦ç”¨ await run_pipelineï¼Œæ”¹ç”¨ add_task
        # é€™æ¨£ API æœƒç«‹åˆ»å›å‚³ "OK"ï¼Œä¸æœƒè®“å‰ç«¯ timeout
        background_tasks.add_task(run_pipeline, save_path, case_name)
        
        return {"status": "processing_started", "case_name": case_name, "message": "Pipeline started in background"}

    except Exception as e:
        print(f"Upload Error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/status/{case_name}")
async def get_status(case_name: str):
    """å‰ç«¯é€éè¼ªè©¢ (Polling) é€™å€‹ API ä¾†ç²å–é€²åº¦"""
    status = file_manager.get_status(case_name)
    return status

@app.get("/api/export/{case_name}/{dataset_type}")
async def export_dataset(case_name: str, dataset_type: str):
    """
    åŒ¯å‡ºåˆä½µå¾Œçš„è³‡æ–™é›†
    """
    # å°æ‡‰ä½ çš„æª”æ¡ˆå¾Œç¶´
    suffix_map = {
        "whisper": "_whisper.json", # Raw ASR (ç›¸å°æ™‚é–“ï¼Œåƒ…ä¾›åƒè€ƒ)
        "diar": "_diar.json",       # Raw Diarization
        "aligned": "_aligned.json", # åˆæ­¥å°é½Š
        "stitched": "_stitched.json", # æ–·å¥ä¿®å¾©å¾Œ
        "flagged": "_flagged_for_human.json", # LLM æ¨™è¨˜å¾Œ
        "edited": "_edited.json"    # äººå·¥ä¿®æ­£ç‰ˆ (é»ƒé‡‘è³‡æ–™)
    }
    
    suffix = suffix_map.get(dataset_type)
    if not suffix:
        raise HTTPException(status_code=400, detail="Unknown dataset type")

    # åŸ·è¡Œåˆä½µ
    merged_data = file_manager.merge_chunks(case_name, suffix)
    
    if not merged_data:
        raise HTTPException(status_code=404, detail=f"No data found for {dataset_type}")

    # è½‰æˆ JSON String
    json_str = json.dumps(merged_data, ensure_ascii=False, indent=2)
    
    # ä¸‹è¼‰æª”åç¯„ä¾‹: 20250324_é™ˆèŠ®æ™_FULL_edited.json
    filename = f"{case_name}_FULL_{dataset_type}.json"
    
    return StreamingResponse(
        io.BytesIO(json_str.encode("utf-8")),
        media_type="application/json",
        headers={"Content-Disposition": f"attachment; filename={filename}"}
    )

if __name__ == "__main__":
    import uvicorn
    uvicorn.run("main:app", host="0.0.0.0", port=8001, reload=True)