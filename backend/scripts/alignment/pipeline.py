import os
import json
import torch
import math
from dotenv import load_dotenv

# --- 1. è¨­å®šæ¨¡å‹è·¯å¾‘ (æœ€é—œéµçš„ä¸€æ­¥) ---
# è¼‰å…¥ .env (ç¢ºä¿è£¡é¢æœ‰ HF_TOKEN)
load_dotenv()

# è¨­å®š Pyannote (HuggingFace) çš„ Cache è·¯å¾‘
# Pyannote æœƒå»é€™å€‹è·¯å¾‘ä¸‹çš„ "hub" è³‡æ–™å¤¾æ‰¾æ¨¡å‹
MODEL_CACHE_DIR = os.getenv("MODEL_CACHE_DIR")
os.environ["HF_HOME"] = MODEL_CACHE_DIR

# è¨­å®š Faster-Whisper çš„æ¨¡å‹è·¯å¾‘
# æ‚¨çš„è³‡æ–™å¤¾åç¨±æ˜¯ "models--Systran--faster-whisper-large-v3"
# é€™é€šå¸¸æ˜¯ huggingface cache çš„çµæ§‹ï¼Œä½† faster-whisper å¯ä»¥ç›´æ¥æŒ‡å®šè·¯å¾‘
WHISPER_MODEL_PATH = os.path.join(MODEL_CACHE_DIR, "models--Systran--faster-whisper-large-v3")

print(f"ğŸ“ Model Root: {MODEL_CACHE_DIR}")
print(f"ğŸ“ Whisper Path: {WHISPER_MODEL_PATH}")

# å»¶é² import
from faster_whisper import WhisperModel
from pyannote.audio import Pipeline

class PipelinePhase2:
    def __init__(self, device="cuda"):
        self.device = device
        self.compute_type = "float16" if torch.cuda.is_available() else "int8"
        
        # æª¢æŸ¥ HF_TOKEN
        if not os.getenv("HF_TOKEN"):
            print("âš ï¸ è­¦å‘Š: æœªåµæ¸¬åˆ° HF_TOKENï¼ŒPyannote å¯èƒ½æœƒå ±éŒ¯ã€‚")

    # --- Step 1: Whisper ---
    def run_whisper(self, audio_path, output_json_path):
        if os.path.exists(output_json_path):
            print(f"â© Whisper output exists, skipping.")
            return

        print(f"ğŸ§ [Step 1] Running Whisper on {os.path.basename(audio_path)}...")
        
        # é€™è£¡æœ‰å…©å€‹ç­–ç•¥ï¼š
        # 1. å¦‚æœ D:\hf_models\models--Systran... è£¡é¢æ˜¯ç›´æ¥çš„æ¨¡å‹æª” (config.json, model.bin)ï¼Œç›´æ¥è®€å–ã€‚
        # 2. å¦‚æœé‚£æ˜¯ç©ºçš„æˆ–çµæ§‹ä¸å°ï¼Œæˆ‘å€‘æŒ‡å› D:\hf_models è®“å®ƒè‡ªå‹•ä¸‹è¼‰/é©—è­‰ã€‚
        
        try:
            # å˜—è©¦ç›´æ¥è®€å–æ‚¨ç¾æœ‰çš„è³‡æ–™å¤¾
            model = WhisperModel(
                WHISPER_MODEL_PATH, 
                device=self.device, 
                compute_type=self.compute_type,
                local_files_only=True # å¼·åˆ¶ä¸è¯ç¶²ï¼Œåªè®€æœ¬åœ°
            )
            print("   (Loading from local path successfully)")
        except Exception as e:
            print(f"   âš ï¸ Local load failed ({e}), falling back to standard loader...")
            # å¦‚æœå¤±æ•—ï¼Œæ”¹ç”¨æ¨™æº–è®€å– (å®ƒæœƒå» D:\hf_models ä¸‹è¼‰æˆ–æ‰¾å¿«å–)
            model = WhisperModel(
                "large-v3", 
                device=self.device, 
                compute_type=self.compute_type,
                download_root=MODEL_CACHE_DIR 
            )

        segments, info = model.transcribe(
            audio_path,
            beam_size=5,
            word_timestamps=True,
            vad_filter=True,
            language="zh"
        )
        
        results = []
        for seg in segments:
            results.append({
                "start": seg.start,
                "end": seg.end,
                "text": seg.text,
                "words": [{"start": w.start, "end": w.end, "word": w.word} for w in seg.words]
            })
            
        with open(output_json_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… Whisper done. Saved to {output_json_path}")
        del model
        torch.cuda.empty_cache()

    # --- Step 2: Pyannote ---
    def run_diarization(self, audio_path, output_json_path):
        if os.path.exists(output_json_path):
            print(f"â© Diarization output exists, skipping.")
            return

        print(f"ğŸ—£ï¸ [Step 2] Running Pyannote on {os.path.basename(audio_path)}...")
        
        # Pyannote çš„è¼‰å…¥æ¯”è¼ƒ trickyï¼Œå®ƒä¾è³´ HF_HOME ç’°å¢ƒè®Šæ•¸
        # æˆ‘å€‘åœ¨ç¨‹å¼æœ€ä¸Šæ–¹å·²ç¶“è¨­å®š os.environ["HF_HOME"] = "D:\hf_models"
        
        hf_token = os.getenv("HF_TOKEN")
        try:
            pipeline = Pipeline.from_pretrained(
                "pyannote/speaker-diarization-3.1",
                use_auth_token=hf_token,
                cache_dir=MODEL_CACHE_DIR # æ˜ç¢ºæŒ‡å®š cache ç›®éŒ„
            ).to(torch.device(self.device))
        except Exception as e:
            print(f"âŒ Pyannote loading failed: {e}")
            print("è«‹ç¢ºèª D:\\hf_models ä¸‹æ˜¯å¦æœ‰ 'models--pyannote--speaker-diarization-3.1' çµæ§‹")
            return

        diarization = pipeline(audio_path)

        diar_segments = []
        for turn, _, speaker in diarization.itertracks(yield_label=True):
            diar_segments.append({
                "start": turn.start,
                "end": turn.end,
                "speaker": speaker
            })
            
        with open(output_json_path, 'w', encoding='utf-8') as f:
            json.dump(diar_segments, f, ensure_ascii=False, indent=2)

        print(f"âœ… Diarization done. Saved to {output_json_path}")
        del pipeline
        torch.cuda.empty_cache()

    # --- Step 3: é‚è¼¯å°é½Š (åŒå‰) ---
    def run_alignment(self, whisper_json, diar_json, final_output_path, chunk_offset_sec=0):
        print(f"ğŸ”— [Step 3] Aligning text with speakers...")
        
        # ç°¡å–®æª¢æŸ¥è¼¸å…¥æª”æ˜¯å¦å­˜åœ¨
        if not os.path.exists(whisper_json) or not os.path.exists(diar_json):
            print("âŒ Input JSONs missing. Please run Step 1 & 2 first.")
            return

        with open(whisper_json, 'r', encoding='utf-8') as f:
            w_segs = json.load(f)
        with open(diar_json, 'r', encoding='utf-8') as f:
            d_segs = json.load(f)
            
        aligned_data = []
        for idx, w in enumerate(w_segs):
            w_start = w["start"]
            w_end = w["end"]
            speaker_scores = {}
            for d in d_segs:
                inter_start = max(w_start, d["start"])
                inter_end = min(w_end, d["end"])
                if inter_end > inter_start:
                    duration = inter_end - inter_start
                    spk = d["speaker"]
                    speaker_scores[spk] = speaker_scores.get(spk, 0) + duration
            
            if speaker_scores:
                best_speaker = max(speaker_scores, key=speaker_scores.get)
            else:
                best_speaker = "Unknown"

            aligned_data.append({
                "id": f"chunk_{int(chunk_offset_sec)}_{idx}",
                "start": round(w_start + chunk_offset_sec, 2),
                "end": round(w_end + chunk_offset_sec, 2),
                "speaker": best_speaker,
                "text": w["text"].strip(),
                "flag": "review_needed" if best_speaker == "Unknown" else "auto"
            })
            
        with open(final_output_path, 'w', encoding='utf-8') as f:
            json.dump(aligned_data, f, ensure_ascii=False, indent=2)
        print(f"ğŸ‰ Final aligned data saved to {final_output_path}")

# --- åŸ·è¡Œå€å¡Š ---
if __name__ == "__main__":
    # é€™è£¡è«‹å¡«å…¥æ‚¨æƒ³è¦è™•ç†çš„é‚£å€‹ WAV æª”æ¡ˆ
    target_wav = "data/temp_chunks/chunk_1_0_531989.wav" 
    
    # ç”¢ç”Ÿæª”å
    base_name = os.path.splitext(target_wav)[0]
    json_whisper = f"{base_name}_whisper.json"
    json_diar = f"{base_name}_diar.json"
    json_final = f"{base_name}_aligned.json"
    
    # è§£æ offset
    try:
        start_ms = int(base_name.split('_')[-2]) 
        offset_sec = start_ms / 1000.0
    except:
        offset_sec = 0

    processor = PipelinePhase2()
    
    # ä¾åºåŸ·è¡Œ
    processor.run_whisper(target_wav, json_whisper)
    processor.run_diarization(target_wav, json_diar)
    processor.run_alignment(json_whisper, json_diar, json_final, offset_sec)