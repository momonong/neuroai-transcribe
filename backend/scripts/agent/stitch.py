import json
import os
import time
from typing import List
from pydantic import BaseModel, Field
import instructor
from llama_cpp import Llama
from dotenv import load_dotenv

load_dotenv()

# --- è¨­å®šè·¯å¾‘ ---
# è«‹ç¢ºèªé€™ä¹Ÿæ˜¯ä½ ä¸‹è¼‰æ¨¡åž‹çš„å¯¦éš›è·¯å¾‘
MODEL_PATH = r"D:/hf_models/Meta-Llama-3.1-8B-Instruct-GGUF/Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf"
INPUT_FILE = "data/temp_chunks/chunk_3_1100278_1606067_aligned.json"
OUTPUT_FILE = "data/temp_chunks/chunk_3_1100278_1606067_stitched_pro.json"

# --- 1. å®šç¾©è³‡æ–™çµæ§‹ (The Schema) ---
# é€™å°±æ˜¯è®“ Google é¢è©¦å®˜é»žé ­çš„é—œéµï¼šå¼·åž‹åˆ¥å®šç¾©
# æˆ‘å€‘å‘Šè¨´æ¨¡åž‹ï¼šä½ åªèƒ½å¡«é€™å€‹è¡¨ï¼Œä¸èƒ½äº‚èªªè©±

class MergeGroup(BaseModel):
    ids: List[int] = Field(
        ..., 
        description="A list of segment IDs (integers) that form ONE complete sentence."
    )

class MergePlan(BaseModel):
    groups: List[MergeGroup] = Field(
        ..., 
        description="A list of merge groups. Covers all segments in the batch."
    )

# --- 2. åˆå§‹åŒ– AI å¼•æ“Ž (GGUF + Instructor) ---
print(f"ðŸ¤– Initializing Llama 3.1 (GGUF) from: {MODEL_PATH}")

try:
    # n_gpu_layers=-1 ä»£è¡¨æŠŠæ‰€æœ‰å±¤éƒ½ä¸Ÿé€² GPU (5090 è·‘ Q4 æ¨¡åž‹ç¶½ç¶½æœ‰é¤˜)
    # n_ctx=8192 æ˜¯ä¸Šä¸‹æ–‡è¦–çª—å¤§å°
    llm = Llama(
        model_path=MODEL_PATH,
        n_gpu_layers=-1, 
        n_ctx=8192,
        verbose=False # é—œé–‰åº•å±¤å›‰å—¦çš„ log
    )
    
    # Patching: è³¦äºˆ Llama çµæ§‹åŒ–è¼¸å‡ºçš„èƒ½åŠ›
    # Patching: è³¦äºˆ Llama çµæ§‹åŒ–è¼¸å‡ºçš„èƒ½åŠ›
    # ä¿®æ”¹ï¼šä½¿ç”¨ Mode.MD_JSONï¼Œé€™å° Llama 3 æ¯”è¼ƒå‹å–„ï¼Œå®¹è¨±å®ƒè¼¸å‡º Markdown
    agent = instructor.patch(
        create=llm.create_chat_completion_openai_v1,
        mode=instructor.Mode.MD_JSON 
    )
    print("âœ… Engine loaded successfully. GPU Acceleration enabled.")

except Exception as e:
    print(f"âŒ Engine load failed: {e}")
    print("è«‹ç¢ºèªæ¨¡åž‹è·¯å¾‘æ˜¯å¦æ­£ç¢ºï¼Œæˆ–æª¢æŸ¥ llama-cpp-python å®‰è£ã€‚")
    exit()

# --- 3. æ ¸å¿ƒé‚è¼¯ï¼šAgent æ±ºç­– ---
def get_merge_plan(batch_segments) -> MergePlan:
    """
    V4.1: æ›´ä¿å®ˆçš„ç¸«åˆç­–ç•¥ï¼Œé¿å…å¥å­è®Šå¾—å¤ªé•·
    """
    context_str = ""
    prev_end = 0.0
    
    for i, seg in enumerate(batch_segments):
        # è¨ˆç®—èˆ‡ä¸Šä¸€å¥çš„æ™‚é–“å·® (Gap)
        gap = seg['start'] - prev_end if i > 0 else 0.0
        prev_end = seg['end']
        
        # æŠŠ Gap ç›´æŽ¥ç®—çµ¦ AI çœ‹ï¼Œè®“å®ƒä¸ç”¨è‡ªå·±åšæ¸›æ³•ï¼Œåˆ¤æ–·æ›´ç²¾æº–
        # æ ¼å¼: ID 0: [Gap: 0.5s] [SPEAKER_00] æ–‡å­—
        gap_str = f"{gap:.2f}s" if i > 0 else "N/A"
        context_str += f"ID {seg['id_in_batch']}: [Gap: {gap_str}] [{seg['speaker']}] {seg['text']}\n"

    system_prompt = """
You are a conservative transcript editor. 
Your goal is to fix fragmented words, NOT to create long paragraphs.

**STRICT MERGING RULES:**
1. **Time Limit**: ONLY merge if the Gap is **LESS THAN 0.8 seconds**.
   - If Gap > 0.8s, DO NOT MERGE.
2. **Short & Sweet**: Avoid creating sentences longer than 30 characters.
3. **Punctuation Logic**: If the first segment sounds complete (e.g., ends with "å–”", "å•Š", "å‘¢"), DO NOT MERGE.
4. **Speaker**: NEVER merge different speakers.

**When in doubt, DO NOT MERGE. Keep segments separate.**

Output the JSON structure as shown in the example.
STRICTLY NO COMMENTS inside the JSON.
"""
    
    try:
        resp = agent(
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": f"Decide merge groups for:\n\n{context_str}"}
            ],
            response_model=MergePlan, 
            temperature=0.1, # ä½Žæº«ä¿æŒå†·éœ
            max_tokens=1024,
        )
        return resp
    except Exception as e:
        print(f"\n   âš ï¸ Agent Inference Error: {e}")
        return None
# --- 4. åŸ·è¡Œèˆ‡ç¸«åˆ (The Executioner) ---
def execute_stitching(raw_batch, plan: MergePlan):
    """
    æ ¹æ“š AI çš„è¨ˆç•«ï¼ŒåŸ·è¡ŒçœŸæ­£çš„å­—ä¸²åˆä½µ
    """
    stitched_batch = []
    processed_ids = set()
    
    # å»ºç«‹ ID åˆ° å…§å®¹ çš„æŸ¥æ‰¾è¡¨ (å› ç‚º raw_batch æ˜¯ä¸€å€‹ list)
    # æˆ‘å€‘é€™è£¡æš«æ™‚å‡è¨­ raw_batch çš„ index å°±æ˜¯ IDï¼Œä½†åœ¨ batch è™•ç†ä¸­è¦å°å¿ƒ
    # ç‚ºäº†å®‰å…¨ï¼Œæˆ‘å€‘é‡æ–°æ˜ å°„
    seg_map = {seg['id_in_batch']: seg for seg in raw_batch}

    for group in plan.groups:
        # éŽæ¿¾ç„¡æ•ˆ ID
        valid_ids = [i for i in group.ids if i in seg_map]
        if not valid_ids: continue
        
        # æ¨™è¨˜å·²è™•ç†
        for i in valid_ids: processed_ids.add(i)
        
        # æŠ“å–ç¬¬ä¸€å¥å’Œæœ€å¾Œä¸€å¥çš„æ™‚é–“
        first_seg = seg_map[valid_ids[0]]
        last_seg = seg_map[valid_ids[-1]]
        
        # åˆä½µæ–‡å­—
        combined_text = "".join([seg_map[i]["text"] for i in valid_ids])
        
        # å»ºç«‹æ–°ç‰©ä»¶
        new_seg = {
            "start": first_seg["start"],
            "end": last_seg["end"],
            "speaker": first_seg["speaker"],
            "text": combined_text,
            "source_ids": [seg_map[i]["id"] for i in valid_ids] # ä¿ç•™åŽŸå§‹çš„å…¨åŸŸ ID
        }
        stitched_batch.append(new_seg)
    
    # è™•ç†æ¼ç¶²ä¹‹é­š (Orphans)
    # å¦‚æžœ AI æ¼æŽ‰äº†æŸäº› IDï¼Œæˆ‘å€‘å¿…é ˆæŠŠå®ƒå€‘åŠ å›žä¾†ï¼Œä¸èƒ½æŽ‰è³‡æ–™
    for i in range(len(raw_batch)):
        if i not in processed_ids:
            seg = raw_batch[i]
            stitched_batch.append({
                "start": seg["start"],
                "end": seg["end"],
                "speaker": seg["speaker"],
                "text": seg["text"],
                "source_ids": [seg["id"]]
            })
            
    # ä¾ç…§é–‹å§‹æ™‚é–“é‡æ–°æŽ’åº
    stitched_batch.sort(key=lambda x: x["start"])
    return stitched_batch

# --- ä¸»ç¨‹å¼ ---
def run_pipeline():
    if not os.path.exists(INPUT_FILE):
        print(f"âŒ Input file not found: {INPUT_FILE}")
        return

    print(f"ðŸ“– Reading fragments from: {INPUT_FILE}")
    with open(INPUT_FILE, 'r', encoding='utf-8') as f:
        raw_data = json.load(f)

    total_segments = len(raw_data)
    final_results = []
    
    # è¨­å®š Batch Size
    WINDOW_SIZE = 10 
    
    print(f"ðŸš€ Starting Pro Stitcher on {total_segments} segments...")
    start_time = time.time()

    for i in range(0, total_segments, WINDOW_SIZE):
        # æº–å‚™ Batch
        batch = raw_data[i : i + WINDOW_SIZE]
        
        # ç‚ºæ¯å€‹ Batch åŠ ä¸Šæš«æ™‚çš„ ID (0~9)ï¼Œæ–¹ä¾¿ AI è­˜åˆ¥
        for idx, seg in enumerate(batch):
            seg['id_in_batch'] = idx
            
        print(f"   Processing Batch {i//WINDOW_SIZE + 1}...", end="\r")
        
        # 1. å–å¾—è¨ˆç•«
        plan = get_merge_plan(batch)
        
        # 2. åŸ·è¡Œåˆä½µ
        if plan:
            merged = execute_stitching(batch, plan)
            final_results.extend(merged)
        else:
            # Fallback: å¦‚æžœ AI çœŸçš„å£žäº†ï¼Œä¿ç•™åŽŸæ¨£
            # (æ³¨æ„ï¼šè¦ç§»é™¤æˆ‘å€‘å‰›å‰›åŠ çš„ id_in_batch æ¬„ä½)
            clean_batch = []
            for seg in batch:
                s = seg.copy()
                s.pop('id_in_batch', None)
                clean_batch.append(s)
            final_results.extend(clean_batch)

    # å­˜æª”
    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
        json.dump(final_results, f, ensure_ascii=False, indent=2)

    end_time = time.time()
    reduction = (1 - len(final_results)/total_segments) * 100
    
    print(f"\n\nâœ¨ Mission Complete!")
    print(f"â±ï¸ Time Taken: {end_time - start_time:.2f}s")
    print(f"ðŸ“‰ Reduction: {total_segments} -> {len(final_results)} segments ({reduction:.1f}%)")
    print(f"ðŸ’¾ Saved to: {OUTPUT_FILE}")

if __name__ == "__main__":
    run_pipeline()