import torch
import time
import librosa
import os
import json # æ–°å¢ json æ¨¡çµ„
from transformers import pipeline
from dotenv import load_dotenv

load_dotenv()

# ==========================================
# 1. è¨­å®šåƒæ•¸èˆ‡è·¯å¾‘
# ==========================================

WHISPER_MODEL_PATH = os.getenv("WHISPER_MODEL_PATH")
LONG_AUDIO_FILE_PATH = "data/ASD/20250324-20054665é™³èŠ®æ™.mp3" 
# è¨­ç½®è½‰éŒ„çµæœçš„å„²å­˜è·¯å¾‘ (æ”¹ç‚º JSON æ ¼å¼)
TRANSCRIPT_OUTPUT_PATH = "data/text/full_whisper_transcript_with_timestamps.json" 

TARGET_DURATION_SECONDS = 60 * 36 # 36 åˆ†é˜
DEVICE = 0 if torch.cuda.is_available() else -1 # ä½¿ç”¨ GPU 0

if not os.path.isdir(WHISPER_MODEL_PATH):
    print(f"âŒ éŒ¯èª¤ï¼šæ‰¾ä¸åˆ°æ¨¡å‹è·¯å¾‘ '{WHISPER_MODEL_PATH}'ã€‚è«‹æª¢æŸ¥è·¯å¾‘æ˜¯å¦æ­£ç¢ºã€‚")
    exit()

# ==========================================
# 2. éŸ³é »è¼‰å…¥ (å®Œæ•´è¼‰å…¥)
# ==========================================
try:
    print(f"ğŸ”„ æ­£åœ¨ä½¿ç”¨ librosa è¼‰å…¥å®Œæ•´çš„ {TARGET_DURATION_SECONDS/60:.0f} åˆ†é˜éŸ³é »æª”æ¡ˆ...")
    audio, sr = librosa.load(
        LONG_AUDIO_FILE_PATH, 
        sr=16000, 
    )
    actual_length = audio.shape[0] / sr
    print(f"âœ… éŸ³é »è¼‰å…¥æˆåŠŸ (å¯¦éš›é•·åº¦: {actual_length:.2f} ç§’)")
except Exception as e:
    print(f"âŒ è¼‰å…¥éŸ³é »æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
    exit()

# ==========================================
# 3. è¨­å®š Pipeline èˆ‡æ¨è«– (é—œéµä¿®æ­£å€å¡Š)
# ==========================================
print(f"ğŸ”„ æ­£åœ¨è¨­å®š Whisper Large v3 Pipeline (ä½¿ç”¨ GPU: {DEVICE})...")
try:
    pipe = pipeline(
        "automatic-speech-recognition",
        model=WHISPER_MODEL_PATH,
        device=DEVICE,
        dtype=torch.float16,
        language='zh',
    )
    print("âœ… Whisper Pipeline è¨­å®šæˆåŠŸã€‚")
except Exception as e:
    print(f"âŒ Pipeline è¨­å®šå¤±æ•—: {e}")
    exit()

print("â³ é–‹å§‹æ¨è«– 36 åˆ†é˜éŸ³é » (å«æ™‚é–“æ¨™è¨˜)...")
start_time = time.time()

# é—œéµä¿®æ­£ï¼š return_timestamps=Trueï¼Œä¸¦ç§»é™¤ chunk_length_s
result = pipe(
    audio, 
    return_timestamps=True, 
    # ä¸éœ€æŒ‡å®š chunk_length_s
)

end_time = time.time()
inference_time = end_time - start_time

# ==========================================
# 4. è¼¸å‡ºçµæœèˆ‡æ€§èƒ½æŒ‡æ¨™ (ä¿®æ­£è¼¸å‡ºæ ¼å¼)
# ==========================================
print("\n--- æ¨è«–çµæœèˆ‡æª”æ¡ˆå„²å­˜ ---")

# é€™è£¡ result æ˜¯ä¸€å€‹å­—å…¸ï¼ŒåŒ…å« 'text' å’Œ 'chunks'
full_transcript_data = result['chunks']

# å¯«å…¥ JSON æª”æ¡ˆ
try:
    with open(TRANSCRIPT_OUTPUT_PATH, "w", encoding="utf-8") as f:
        # ä½¿ç”¨ json.dump å„²å­˜å¸¶æœ‰æ™‚é–“æ¨™è¨˜çš„çµæ§‹åŒ–æ•¸æ“š
        json.dump(full_transcript_data, f, ensure_ascii=False, indent=4)
    print(f"ğŸ’¾ å®Œæ•´çµæ§‹åŒ–é€å­—ç¨¿å·²å„²å­˜è‡³: {TRANSCRIPT_OUTPUT_PATH}")
except Exception as e:
    print(f"âŒ å„²å­˜æª”æ¡ˆå¤±æ•—: {e}")

# æ€§èƒ½æŒ‡æ¨™
print("\n--- æ€§èƒ½æŒ‡æ¨™ ---")
print(f"âœ… æ¨è«–å®Œæˆï¼ç¸½è€—æ™‚: {inference_time:.2f} ç§’")
print(f"â±ï¸ å¯¦éš›å³æ™‚ç‡ (RTF): {inference_time / actual_length:.2f}x")
print(f"ğŸ“ åˆæ­¥é€å­—ç¨¿ç‰‡æ®µ (å‰ 5 å¥):")
# è¼¸å‡ºå‰ 5 å¥ï¼Œé¡¯ç¤ºæ™‚é–“æ¨™è¨˜
for i, chunk in enumerate(full_transcript_data[:5]):
    start_time_s = chunk['timestamp'][0]
    end_time_s = chunk['timestamp'][1]
    print(f"  [{start_time_s:.2f}s - {end_time_s:.2f}s] {chunk['text']}")
    if i == 4: break
print("-" * 50)