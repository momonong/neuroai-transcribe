import json
import re
import os
import jieba
import jieba.analyse
from collections import Counter

# ==========================================
# è¨­å®š
# ==========================================
GT_SRT_FILE = "data\ASD\GTruth.srt" 
AI_JSON_FILE = "data/db/final_web_ready_script.json"
RAW_JSON_FILE = "data/text/full_whisper_transcript_with_timestamps.json"

def clean_text(text):
    text = re.sub(r'(å°å­©|æ¸¬è©¦è€…|è€å¸«|Child|Therapist|Unknown)[:ï¼š]\s*', '', text)
    text = re.sub(r'[^\u4e00-\u9fa5]', '', text)
    return text

def get_text_from_file(file_type):
    text = ""
    if file_type == "GT":
        with open(GT_SRT_FILE, 'r', encoding='utf-8') as f:
            content = f.read()
            lines = [l for l in content.split('\n') if "-->" not in l and not l.strip().isdigit()]
            text = "".join([clean_text(l) for l in lines])
    elif file_type == "RAW":
        with open(RAW_JSON_FILE, 'r', encoding='utf-8') as f:
            data = json.load(f)
            segs = data if isinstance(data, list) else data.get('segments', [])
            text = "".join([clean_text(s['text']) for s in segs])
    elif file_type == "AI":
        with open(AI_JSON_FILE, 'r', encoding='utf-8') as f:
            data = json.load(f)
            text = "".join([clean_text(item['text']) for item in data])
    return text

def calc_repetition_rate(text, n=4):
    """è¨ˆç®— N-gram é‡è¤‡ç‡ (æª¢æ¸¬è·³é‡)"""
    if len(text) < n: return 0.0
    ngrams = [text[i:i+n] for i in range(len(text)-n+1)]
    counts = Counter(ngrams)
    # é‡è¤‡å‡ºç¾è¶…é 1 æ¬¡çš„ ngram æ•¸é‡
    repeated_ngrams = sum(count for gram, count in counts.items() if count > 1)
    return repeated_ngrams / len(ngrams) if ngrams else 0

def calc_keyword_recall(gt_text, hyp_text, top_k=100):
    """è¨ˆç®—é—œéµè©å¬å›ç‡"""
    # 1. å¾ Ground Truth æå–æœ€é‡è¦çš„ K å€‹é—œéµè© (TF-IDF)
    keywords = jieba.analyse.extract_tags(gt_text, topK=top_k)
    
    # 2. æª¢æŸ¥é€™äº›è©æœ‰æ²’æœ‰åœ¨ Hypothesis è£¡å‡ºç¾
    hit_count = 0
    for kw in keywords:
        if kw in hyp_text:
            hit_count += 1
            
    return hit_count / len(keywords) * 100, keywords

def run_advanced_eval():
    print("ğŸš€ é–‹å§‹è¨ˆç®—é€²éšæŒ‡æ¨™ (Signal-to-Noise)...")
    
    gt_text = get_text_from_file("GT")
    raw_text = get_text_from_file("RAW")
    ai_text = get_text_from_file("AI")
    
    print(f"å­—æ•¸çµ±è¨ˆ: GT={len(gt_text)}, Raw={len(raw_text)}, AI={len(ai_text)}")
    print("-" * 50)
    
    # 1. é‡è¤‡ç‡æ¯”è¼ƒ (è¶Šä½è¶Šå¥½ -> ä»£è¡¨æ²’æœ‰å¹»è¦ºè¿´åœˆ)
    rep_raw = calc_repetition_rate(raw_text, n=4) * 100
    rep_ai = calc_repetition_rate(ai_text, n=4) * 100
    
    print(f"ğŸ”„ 4-gram é‡è¤‡ç‡ (Repetition Rate) [è¶Šä½è¶Šå¥½]")
    print(f"   Baseline (Raw): {rep_raw:.2f}%")
    print(f"   Ours (Agent):   {rep_ai:.2f}%")
    if rep_ai < rep_raw:
        print(f"   âœ… æ”¹å–„: é™ä½äº† {rep_raw - rep_ai:.2f}% çš„æ©Ÿæ¢°æ€§é‡è¤‡ (å¹»è¦ºæ¶ˆé™¤)")
    else:
        print("   âš ï¸ æœªé¡¯è‘—é™ä½")
        
    print("-" * 50)
    
    # 2. é—œéµè©ä¿ç•™ç‡ (è¶Šé«˜è¶Šå¥½ -> ä»£è¡¨æ²’æœ‰èª¤åˆªé‡è¦è³‡è¨Š)
    # æˆ‘å€‘å–å‰ 200 å€‹é‡è¦è©å½™
    kw_recall_raw, keywords = calc_keyword_recall(gt_text, raw_text, top_k=200)
    kw_recall_ai, _ = calc_keyword_recall(gt_text, ai_text, top_k=200)
    
    print(f"ğŸ¯ é—œéµè©å¬å›ç‡ (Keyword Recall) [è¶Šé«˜è¶Šå¥½]")
    print(f"   Baseline (Raw): {kw_recall_raw:.1f}%")
    print(f"   Ours (Agent):   {kw_recall_ai:.1f}%")
    
    print("-" * 50)
    print("ğŸ’¡ çµè«–å»ºè­°:")
    
    if rep_ai < rep_raw and kw_recall_ai >= (kw_recall_raw - 5):
        print("ğŸ‰ å®Œç¾åŠ‡æœ¬ï¼")
        print("   è«–é»ï¼šæˆ‘å€‘çš„ç³»çµ±å¤§å¹…é™ä½äº†é›œè¨Š (é‡è¤‡ç‡ä¸‹é™)ï¼Œ")
        print("   åŒæ™‚å®Œç¾ä¿ç•™äº†è‡¨åºŠé—œéµè³‡è¨Š (é—œéµè©å¬å›ç‡æŒå¹³)ã€‚")
        print("   é€™è­‰æ˜äº†æˆ‘å€‘æé«˜äº†ã€è³‡è¨Šå¯†åº¦ã€ï¼")

if __name__ == "__main__":
    run_advanced_eval()