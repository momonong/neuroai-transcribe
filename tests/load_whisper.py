import torch
import time
import librosa
import os
from transformers import pipeline
from dotenv import load_dotenv

load_dotenv()

# ==========================================
# 1. è¨­å®šåƒæ•¸èˆ‡è·¯å¾‘
# ==========================================

# æ›¿æ›æˆä½ å¯¦éš›çš„æ¨¡å‹è·¯å¾‘
WHISPER_MODEL_PATH = os.getenv("WHISPER_MODEL_PATH")
# æ›¿æ›æˆä½ å¯¦éš›çš„éŸ³é »æª”æ¡ˆè·¯å¾‘
LONG_AUDIO_FILE_PATH = "data/20250324-20054665é™³èŠ®æ™.mp3" 

TARGET_DURATION_SECONDS = 60 * 36 # 3 åˆ†é˜
DEVICE = 0 if torch.cuda.is_available() else -1 # ä½¿ç”¨ GPU 0

# ç¢ºä¿è·¯å¾‘å­˜åœ¨
if not os.path.isdir(WHISPER_MODEL_PATH):
    print(f"âŒ éŒ¯èª¤ï¼šæ‰¾ä¸åˆ°æ¨¡å‹è·¯å¾‘ '{WHISPER_MODEL_PATH}'ã€‚è«‹æª¢æŸ¥è·¯å¾‘æ˜¯å¦æ­£ç¢ºã€‚")
    exit()

# ==========================================
# 2. éŸ³é »è¼‰å…¥ (åƒ…è¼‰å…¥å‰ 3 åˆ†é˜)
# ==========================================
try:
    print(f"ğŸ”„ æ­£åœ¨ä½¿ç”¨ librosa è¼‰å…¥éŸ³é »æª”æ¡ˆçš„å‰ {TARGET_DURATION_SECONDS} ç§’...")
    # sr=16000 æ˜¯ Whisper è¦æ±‚æ¡æ¨£ç‡
    audio, sr = librosa.load(
        LONG_AUDIO_FILE_PATH, 
        sr=16000, 
        # duration=TARGET_DURATION_SECONDS # é—œéµï¼šè¨­å®šè¼‰å…¥é•·åº¦
    )
    actual_length = audio.shape[0] / sr
    print(f"âœ… éŸ³é »è¼‰å…¥æˆåŠŸ (å¯¦éš›é•·åº¦: {actual_length:.2f} ç§’)")
except FileNotFoundError:
    print(f"âŒ éŒ¯èª¤ï¼šæ‰¾ä¸åˆ°éŸ³é »æª”æ¡ˆã€‚è«‹å°‡ '{LONG_AUDIO_FILE_PATH}' æ›¿æ›ç‚ºå¯¦éš›æª”æ¡ˆè·¯å¾‘ã€‚")
    exit()
except Exception as e:
    print(f"âŒ è¼‰å…¥éŸ³é »æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
    exit()

# ==========================================
# 3. è¨­å®š Pipeline èˆ‡æ¨è«–
# ==========================================
print(f"ğŸ”„ æ­£åœ¨è¨­å®š Whisper Large v3 Pipeline (ä½¿ç”¨ GPU: {DEVICE})...")
try:
    # ä½¿ç”¨ pipeline ç°¡åŒ–æµç¨‹ï¼Œä¸¦ç”¨ float16 æ¸›å°‘ VRAM ä½”ç”¨
    pipe = pipeline(
        "automatic-speech-recognition",
        model=WHISPER_MODEL_PATH,
        device=DEVICE,
        dtype=torch.float16,
        language='zh',
    )
    print("âœ… Whisper Pipeline è¨­å®šæˆåŠŸã€‚")
except Exception as e:
    print(f"âŒ Pipeline è¨­å®šå¤±æ•—: {e}")
    exit()

print("â³ é–‹å§‹æ¨è«– 36 åˆ†é˜éŸ³é »...")
start_time = time.time()

# é‹è¡Œæ¨è«– (ä½¿ç”¨åˆ†å¡Šè™•ç†ï¼Œé¿å…å–®æ¬¡è¼¸å…¥éå¤§)
result = pipe(audio, chunk_length_s=30, return_timestamps=False)

end_time = time.time()
inference_time = end_time - start_time

# ==========================================
# 4. è¼¸å‡ºçµæœèˆ‡æ€§èƒ½æŒ‡æ¨™
# ==========================================
print("\n--- æ¨è«–çµæœèˆ‡è³‡æºæ¶ˆè€—åˆ†æ ---")
print(f"âœ… æ¨è«–å®Œæˆï¼ç¸½è€—æ™‚: {inference_time:.2f} ç§’")
print(f"â±ï¸ å¯¦éš›å³æ™‚ç‡ (RTF): {inference_time / actual_length:.2f}x (æ•¸å€¼è¶Šä½è¶Šå¥½)")
print(f"ğŸ“ åˆæ­¥é€å­—ç¨¿ç‰‡æ®µ: {result['text'][:200]}...")
print("-" * 50)

# è³‡æºç›£æ§æé†’
print("ğŸ’¡ è³‡æºæ¶ˆè€—æé†’ï¼š")
print("åœ¨æ¨¡å‹é‹è¡Œæ™‚ï¼ŒWhisper Large v3 (FP16) ç´„ä½”ç”¨ 9-11 GB VRAMã€‚")
print(f"é€™å€‹ RTF ({inference_time / actual_length:.2f}x) æ•¸å­—å°‡æ±ºå®šæ‚¨æœªä¾† 36 åˆ†é˜éŸ³é »çš„ç¸½è™•ç†æ™‚é–“ï¼š")
print(f"  é è¨ˆç¸½è™•ç†æ™‚é–“ç´„ç‚º: {inference_time / actual_length * 36 * 60 / 60:.1f} åˆ†é˜ã€‚")