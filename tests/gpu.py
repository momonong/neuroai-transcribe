import whisperx
from whisperx.diarize import DiarizationPipeline
import json
import os
import torch
import time
from dotenv import load_dotenv

# ==========================================
# ğŸš‘ã€çµ‚æ¥µæš´åŠ›ä¿®æ­£ã€‘å¼·åˆ¶é—œé–‰ PyTorch 2.6+ å®‰å…¨æª¢æŸ¥
# ==========================================
# ä¹‹å‰çš„ Patch å¤ªç¦®è²Œäº†ï¼Œç¾åœ¨æˆ‘å€‘ä¸ç®¡å‘¼å«è€…è¦æ±‚ä»€éº¼ï¼Œ
# å¼·åˆ¶å°‡ weights_only è¨­ç‚º Falseã€‚
original_load = torch.load

def aggressive_load(*args, **kwargs):
    # ç›´æ¥è¦†å¯«ï¼Œä¸ç®¡åŸæœ¬å‚³é€²ä¾†ä»€éº¼
    kwargs['weights_only'] = False 
    return original_load(*args, **kwargs)

# æ›¿æ›æ‰ç³»çµ±åŸæœ¬çš„ load å‡½æ•¸
torch.load = aggressive_load
print("ğŸ”§ å·²å¼·åˆ¶è§£é™¤ PyTorch æ¨¡å‹è®€å–é™åˆ¶ (Aggressive Patch Applied)")
# ==========================================

load_dotenv()

# ... (ä»¥ä¸‹æ¥ä½ åŸæœ¬çš„ç¨‹å¼ç¢¼) ...


# ==========================================
# 1. è¨­å®š
# ==========================================
# ä¸éœ€è¦æŒ‡åˆ° config.yaml äº†ï¼Œç›´æ¥ç”¨ HuggingFace Token
HF_TOKEN = os.getenv("HF_TOKEN") 
AUDIO_FILE = os.getenv("AUDIO_FILE")
OUTPUT_JSON = "data/text/stage1_whisperx_aligned.json"

# è¨­å®šé‹ç®—è£ç½® (5090 / 3090 / 4060 ç­‰)
device = "cuda" 
batch_size = 16 # é¡¯å­˜å¤§(24G)å¯ä»¥é–‹åˆ° 16 æˆ– 32ï¼Œé¡¯å­˜å°(16G)é–‹ 8 æˆ– 4
compute_type = "float16" # 5090 çµ•å°æ”¯æ´ float16

print(f"ğŸš€ [Stage 1] å•Ÿå‹•æ„ŸçŸ¥å±¤ (WhisperX Pipeline)...")
print(f" Â  - Device: {device}")
audio_file_show = AUDIO_FILE.replace(os.getenv("TESTER_NAME"), "")
print(f" Â  - Audio: {audio_file_show}")

start_total = time.time()

# ==========================================
# 2. Transcribe (è½‰éŒ„)
# ==========================================
print("\nğŸ“ [Step 1] è¼‰å…¥ Whisper æ¨¡å‹èˆ‡è½‰éŒ„...")
# model_dir å¯ä»¥æŒ‡å®šæœ¬åœ°è·¯å¾‘ï¼Œå¦‚æœä¸æŒ‡å®šå®ƒæœƒè‡ªå‹•ç®¡ç†å¿«å–
model = whisperx.load_model("large-v3", device, compute_type=compute_type, language="zh")

# åŸ·è¡Œè½‰éŒ„
audio = whisperx.load_audio(AUDIO_FILE)
result = model.transcribe(audio, batch_size=batch_size)
print(f"âœ… è½‰éŒ„å®Œæˆ (Segments: {len(result['segments'])})")

# ==========================================
# 3. Alignment (å¼·åˆ¶å°é½Š - é€™æ˜¯åŸæœ¬è…³æœ¬æ²’æœ‰çš„ç¥æŠ€)
# ==========================================
print("\nğŸ“ [Step 2] åŸ·è¡ŒéŸ³ç´ ç´šå¼·åˆ¶å°é½Š (Phoneme Alignment)...")
# é€™ä¸€æ­¥æœƒä¿®æ­£ ASR çš„æ™‚é–“æˆ³ï¼Œè®“å®ƒæº–ç¢ºåˆ°æ¯«ç§’
model_a, metadata = whisperx.load_align_model(language_code=result["language"], device=device)
result = whisperx.align(result["segments"], model_a, metadata, audio, device, return_char_alignments=False)
print("âœ… å°é½Šå®Œæˆ")

# ==========================================
# 4. Diarization (èªè€…åˆ†é›¢)
# ==========================================
print("\nğŸ‘‚ [Step 3] åŸ·è¡Œèªè€…åˆ†é›¢ (Speaker Diarization)...")
# é€™è£¡ç›´æ¥å‘¼å« Pyannote 3.1ï¼Œä¸éœ€è¦æ‰‹å‹•è¼‰å…¥ config.yaml
diarize_model = DiarizationPipeline(use_auth_token=HF_TOKEN, device=device)
diarize_segments = diarize_model(audio)

# æŠŠèªè€… ID åˆ†é…çµ¦å‰›å‰›è½‰éŒ„å¥½çš„æ–‡å­—
# min_speakers å’Œ max_speakers å¯ä»¥å¹«åŠ©æ¨¡å‹æ›´æº–ç¢º (é€šå¸¸ ASD å ´æ™¯å°±æ˜¯ 2-3 äºº)
result = whisperx.assign_word_speakers(diarize_segments, result)
print("âœ… èªè€…åˆ†é…å®Œæˆ")

# ==========================================
# 5. è¼¸å‡ºçµæœèˆ‡æ ¼å¼åŒ–
# ==========================================
print("\nğŸ§  [Orchestrator] æ­£åœ¨æ‰“åŒ…è³‡æ–™...")

final_corpus = []
for seg in result["segments"]:
    final_corpus.append({
        "start": round(seg["start"], 3),
        "end": round(seg["end"], 3),
        "speaker": seg.get("speaker", "UNKNOWN"), # å¦‚æœæ²’æŠ“åˆ°èªè€…æœƒæ¨™ç¤º UNKNOWN
        "text": seg["text"].strip()
    })

os.makedirs(os.path.dirname(OUTPUT_JSON), exist_ok=True)
with open(OUTPUT_JSON, "w", encoding="utf-8") as f:
    json.dump(final_corpus, f, ensure_ascii=False, indent=4)

print("-" * 50)
print(f"ğŸ’¾ [Output] å·²ç”Ÿæˆå°é½Šèªæ–™åº«: {OUTPUT_JSON}")
print(f"ğŸ‰ ç¸½è€—æ™‚: {time.time() - start_total:.1f} ç§’")